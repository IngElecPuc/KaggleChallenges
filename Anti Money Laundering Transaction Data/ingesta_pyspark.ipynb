{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fc68eb-3433-4082-abfd-5616715266d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bfbe4a-c075-4ce3-b364-8b17b3292509",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_URL  = \"jdbc:postgresql://localhost:5432/graphs\"\n",
    "PG_USER = \"spark_ingest\"\n",
    "PG_PASS = \"GYleZAI2pTBKJYl9W1PL\"\n",
    "PG_SCHEMA = \"raw\"\n",
    "#CSV_DIR = r\"E:\\Datasets\\ieee-fraud-detection\"  \n",
    "CSV_DIR = r\"E:\\Datasets\\Anti Money Laundering Transaction Data (SAML-D)\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383d4d49-0d7d-4157-b11c-7a9772eb42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILES = {\n",
    "#    \"train_identity.csv\":     \"train_identity\",\n",
    "#    \"train_transaction.csv\":  \"train_transaction\",\n",
    "#    \"test_identity.csv\":      \"test_identity\",\n",
    "#    \"test_transaction.csv\":   \"test_transaction\",\n",
    "#}\n",
    "FILES = {\"SAML-D.csv\": \"saml_d\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4960d65-84ae-4079-8d68-683b8eb81915",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTITIONS = 6\n",
    "JDBC_BATCHSIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbc27be-9b5c-43f8-9af5-9d26b180db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "JDBC_JAR = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\postgresql-42.7.4.jar\"  # ruta sin espacios si puedes\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ieee-fraud-jupyter\")\n",
    "    .config(\"spark.jars\", JDBC_JAR)\n",
    "    .config(\"spark.driver.extraClassPath\", JDBC_JAR)\n",
    "    .config(\"spark.executor.extraClassPath\", JDBC_JAR)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b455ba4-3f91-499c-86bc-f12a7a044365",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 0)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(max(4, NUM_PARTITIONS)))\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69fff88-5e2f-4169-a223-5f3483dc2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_col(name: str) -> str:\n",
    "    # minúsculas, sustituir espacios y caracteres raros por _\n",
    "    s = name.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9_]\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d23acd4-54ac-483f-9f2d-e4faddf22b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(filename: str):\n",
    "    path = f\"{CSV_DIR}\\\\{filename}\"\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")          # staging rápido; luego puedes tipificar en SQL\n",
    "        .option(\"multiLine\", \"false\")\n",
    "        .option(\"escape\", \"\\\"\")\n",
    "        .option(\"quote\", \"\\\"\")\n",
    "        .option(\"nullValue\", \"\")\n",
    "        .option(\"mode\", \"PERMISSIVE\")\n",
    "        .option(\"maxCharsPerColumn\", \"1000000\") # por si hay campos largos\n",
    "        .csv(path)\n",
    "    )\n",
    "    # normalizar columnas\n",
    "    new_cols = [normalize_col(c) for c in df.columns]\n",
    "    for old, new in zip(df.columns, new_cols):\n",
    "        if old != new:\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "    # Reparticionar para IO balanceado\n",
    "    df = df.repartition(NUM_PARTITIONS)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fdc2043-377c-470d-bfc9-b0e2afc065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pg(df, table_name: str):\n",
    "    # Escribir a schema.table; modo overwrite para idempotencia inicial\n",
    "    full_table = f\"{PG_SCHEMA}.{table_name}\"\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", PG_URL)\n",
    "        .option(\"dbtable\", full_table)\n",
    "        .option(\"user\", PG_USER)\n",
    "        .option(\"password\", PG_PASS)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        # Rendimiento y compatibilidad\n",
    "        .option(\"stringtype\", \"unspecified\")      \n",
    "        .option(\"reWriteBatchedInserts\", \"true\")  \n",
    "        .option(\"batchsize\", str(JDBC_BATCHSIZE))\n",
    "        .option(\"truncate\", \"true\") \n",
    "        .mode(\"overwrite\")\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa65954-3374-4303-8959-6753cfe7e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for fname, tname in FILES.items():\n",
    "        print(f\"==> Cargando {fname} ...\")\n",
    "        df = load_csv(fname)\n",
    "        print(f\"   Columnas: {len(df.columns)} | Registros estimados: {df.count()}\")\n",
    "        print(f\"==> Escribiendo en {PG_SCHEMA}.{tname} ...\")\n",
    "        write_pg(df, tname)\n",
    "        print(f\"   OK: {PG_SCHEMA}.{tname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cd4b4f5-0b2e-4d0c-a6e9-b51b95cda8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Cargando SAML-D.csv ...\n",
      "   Columnas: 12 | Registros estimados: 9504852\n",
      "==> Escribiendo en raw.saml_d ...\n",
      "   OK: raw.saml_d\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0416a1d3-e33a-453c-b4d3-ad01d0f7923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import URL\n",
    "\n",
    "connection_url = URL.create(\n",
    "    drivername='postgresql+psycopg2',\n",
    "    username=PG_USER,\n",
    "    password=PG_PASS,  \n",
    "    host='localhost',\n",
    "    port=5432,\n",
    "    database='graphs',\n",
    "    query={'sslmode': 'require'},\n",
    ")\n",
    "engine = create_engine(connection_url)\n",
    "\n",
    "stmts = [\n",
    "    \"ALTER TABLE raw.saml_d ADD COLUMN IF NOT EXISTS id BIGINT\",\n",
    "    \"DO $$ BEGIN CREATE SEQUENCE raw.saml_d_id_seq; EXCEPTION WHEN duplicate_table THEN NULL; END $$;\",\n",
    "    \"ALTER TABLE raw.saml_d ALTER COLUMN id SET DEFAULT nextval('raw.saml_d_id_seq')\",\n",
    "    \"UPDATE raw.saml_d SET id = nextval('raw.saml_d_id_seq') WHERE id IS NULL\",\n",
    "    \"ALTER TABLE raw.saml_d ALTER COLUMN id SET NOT NULL\",\n",
    "    \"DO $$ BEGIN ALTER TABLE raw.saml_d ADD CONSTRAINT saml_d_pkey PRIMARY KEY (id); \"\n",
    "    \"EXCEPTION WHEN duplicate_object THEN NULL; END $$;\",\n",
    "] #Agregar un id para poder particionar más adelante con pyspark al leer\n",
    "\n",
    "with engine.begin() as conn:  # transacción\n",
    "    for s in stmts:\n",
    "        conn.execute(text(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fae3c7-452e-4b96-8e02-3cf9a7344f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o1003"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._jvm.java.lang.Class.forName(\"org.postgresql.Driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef979ef2-15c4-4aa2-ab25-b20df6e09f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
