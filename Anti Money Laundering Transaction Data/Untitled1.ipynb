{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0d56ca-9442-4618-8ad7-e6a5bbf0df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from ETL_pg2neo4j.load_config import (\n",
    "    CFG, SPARK_LOCAL_DIR, PYTHON, PG_USER, \n",
    "    PG_PASS, JDBC_FETCHSIZE, IS_WIN, LOG_DIR\n",
    ")\n",
    "\n",
    "def _normalize_jars(value):\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        items = list(value)\n",
    "    else:\n",
    "        items = [p.strip() for p in str(value).split(\",\") if str(value).strip()]\n",
    "    return [p for p in items if p and Path(p).exists()]\n",
    "\n",
    "def _jprop(key: str, value: Path | str) -> str:\n",
    "    v = str(value)\n",
    "    if \" \" in v:\n",
    "        v = f'\"{v}\"'\n",
    "    return f\"-D{key}={v}\"\n",
    "\n",
    "def get_spark(stats):\n",
    "    if CFG['spark']['use_packages']:\n",
    "        spark_jars_key = \"spark.jars.packages\"\n",
    "        spark_jars_value = \",\".join(CFG[\"spark\"][\"maven_packages\"])\n",
    "        set_jars = True\n",
    "    else:\n",
    "        spark_jars_key = \"spark.jars\"\n",
    "        local_jars = CFG[\"spark\"][\"local_jars\"][\"windows\" if IS_WIN else \"linux\"]\n",
    "        jars_list = _normalize_jars(local_jars)\n",
    "        set_jars = len(jars_list) > 0\n",
    "        # OJO: spark.jars -> separado por comas\n",
    "        spark_jars_value = \",\".join(jars_list)\n",
    "\n",
    "    #conf_path = str((Path(__file__).resolve().parent / \"config\" / \"log4j2.properties\"))\n",
    "    #driver_opts = \" \".join([\n",
    "    #    _jprop(\"log4j2.configurationFile\", f\"file:{conf_path}\"),\n",
    "    #    _jprop(\"LOG_DIR\", LOG_DIR),\n",
    "    #])\n",
    "    #executor_opts = driver_opts\n",
    "\n",
    "    usable_cores = max(1, stats['cpu_cores'] - 1)\n",
    "\n",
    "    builder = (SparkSession.builder\n",
    "        .appName(\"postgres-to-neo4j-graph\")\n",
    "        .master(f\"local[{usable_cores}]\")\n",
    "        .config(\"spark.pyspark.driver.python\", PYTHON)\n",
    "        .config(\"spark.pyspark.python\", PYTHON)\n",
    "        .config(\"spark.executorEnv.PYSPARK_PYTHON\", PYTHON)\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "        .config(\"spark.python.use.daemon\", \"false\")\n",
    "        .config(\"spark.local.dir\", SPARK_LOCAL_DIR)\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(CFG[\"spark\"][\"shuffle_partitions\"]))\n",
    "        .config(\"spark.driver.memory\", CFG[\"spark\"][\"driver_memory\"])\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        #.config(\"spark.driver.extraJavaOptions\", driver_opts)\n",
    "        #.config(\"spark.executor.extraJavaOptions\", executor_opts)\n",
    "    )\n",
    "\n",
    "    if set_jars and spark_jars_value:\n",
    "        # spark.jars necesita comas\n",
    "        builder = builder.config(spark_jars_key, spark_jars_value)\n",
    "        print(\"[SPARK] usando jars locales ->\", spark_jars_value.replace(',', ':'))\n",
    "\n",
    "        # extraClassPath necesita separador de classpath del SO (':' en Linux)\n",
    "        cp_value = os.pathsep.join(spark_jars_value.split(\",\"))  # <- aquí el cambio\n",
    "        builder = (builder\n",
    "            .config(\"spark.driver.extraClassPath\", cp_value)\n",
    "            .config(\"spark.executor.extraClassPath\", cp_value)\n",
    "        )\n",
    "        print(\"[SPARK] classpath reforzado (driver/executors) ->\", cp_value)\n",
    "    else:\n",
    "        print(\"[SPARK] ADVERTENCIA: no se encontraron jars locales; dependerá de $SPARK_HOME/jars\")\n",
    "\n",
    "    spark = builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "    try:\n",
    "        spark._jvm.java.lang.Class.forName(\"org.postgresql.Driver\")\n",
    "        print(\"[CHECK] Postgres JDBC REALMENTE visible\")\n",
    "    except Exception as e:\n",
    "        print(\"[CHECK] Postgres JDBC NO visible:\", e)\n",
    "\n",
    "    try:\n",
    "        spark._jvm.java.lang.Class.forName(\"org.neo4j.spark.DataSource\")\n",
    "        print(\"[CHECK] Neo4j connector REALMENTE visible\")\n",
    "    except Exception as e:\n",
    "        print(\"[CHECK] Neo4j connector NO visible:\", e)\n",
    "\n",
    "    jdbc_props = {\n",
    "        \"user\": PG_USER,\n",
    "        \"password\": PG_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\",\n",
    "        \"fetchsize\": str(JDBC_FETCHSIZE)\n",
    "    }\n",
    "    return spark, jdbc_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e9afcd5-0444-412b-bbb8-193f1aa05e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, logging, traceback\n",
    "from ETL_pg2neo4j.diagnostics import print_diagnostics\n",
    "from ETL_pg2neo4j.logs import (\n",
    "    StreamToLogger, get_python_logger,\n",
    "    instantiate_pglogs, write_python_logs_to_pg, write_spark_logs_to_pg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d64a5700-b056-4ca5-8333-a07f1425e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_python_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e8fe960-59e4-4120-ac35-e6ee2516afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DIAG] Cores totales          : 8\n",
      "[DIAG] RAM total              : 15.5 GB\n",
      "[DIAG] RAM usada              : 13.2 GB\n",
      "[DIAG] RAM disponible         : 2.3 GB\n"
     ]
    }
   ],
   "source": [
    "stats = print_diagnostics(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f488495e-e42e-44f3-9d2d-4db0ecc2b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPARK] usando jars locales -> /opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar\n",
      "[SPARK] classpath reforzado (driver/executors) -> /opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark, jdbc_props \u001b[38;5;241m=\u001b[39m \u001b[43mget_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 76\u001b[0m, in \u001b[0;36mget_spark\u001b[0;34m(stats)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SPARK] ADVERTENCIA: no se encontraron jars locales; dependerá de $SPARK_HOME/jars\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/UDD/Bank fraud detection/KaggleChallenges/venv/lib/python3.10/site-packages/pyspark/sql/session.py:559\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    561\u001b[0m     module \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(session\u001b[38;5;241m.\u001b[39m_jvm)\n",
      "File \u001b[0;32m~/UDD/Bank fraud detection/KaggleChallenges/venv/lib/python3.10/site-packages/pyspark/sql/session.py:635\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    631\u001b[0m jSparkSessionModule \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_get_j_spark_session_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jsparkSession \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 635\u001b[0m         \u001b[43mjSparkSessionClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39misDefined()\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39msparkContext()\u001b[38;5;241m.\u001b[39misStopped()\n\u001b[1;32m    637\u001b[0m     ):\n\u001b[1;32m    638\u001b[0m         jsparkSession \u001b[38;5;241m=\u001b[39m jSparkSessionClass\u001b[38;5;241m.\u001b[39mgetDefaultSession()\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    639\u001b[0m         jSparkSessionModule\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(jsparkSession, options)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "spark, jdbc_props = get_spark(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3c807-5d20-4719-8ba4-85d4d31c25ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
