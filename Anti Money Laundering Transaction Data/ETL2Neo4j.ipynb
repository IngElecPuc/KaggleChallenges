{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8e7c6d-9ca9-438c-a6cb-d130d992f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores disponibles: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from py2neo import Graph\n",
    "import sys, os\n",
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(f\"CPU cores disponibles: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c380f8-3f18-4fc5-98d9-da76d31e290e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lectura de dataframes desde PostGreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f5ac2-db91-45c9-9eae-a3e0cdec90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Credenciales ===\n",
    "PG_URL  = 'jdbc:postgresql://localhost:5432/graphs'\n",
    "PG_USER = 'spark_ingest'\n",
    "PG_PASS = 'GYleZAI2pTBKJYl9W1PL'\n",
    "PG_SCHEMA = 'saml_d'\n",
    "PG_TABLE1 = 'accounts'\n",
    "PG_TABLE2 = 'transferences'\n",
    "PG_TABLE3 = 'statements'\n",
    "\n",
    "JDBC_JAR = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\postgresql-42.7.4.jar\"\n",
    "JDBC_BATCHSIZE = 10000\n",
    "JDBC_FETCHSIZE = 10000\n",
    "\n",
    "NEO4J_JAR  = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\neo4j-connector-apache-spark_2.13-5.3.11-SNAPSHOT_for_spark_3.jar\"\n",
    "NEO4J_URI  = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"Banco.69\"\n",
    "NEO4J_DDBB = \"saml-d\"\n",
    "\n",
    "PYTHON = sys.executable  # python del kernel Jupyter\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"postgres-to-neo4j-graph\")\n",
    "    .master(\"local[*]\")\n",
    "    # === JARs locales ===\n",
    "    .config(\"spark.jars\", f\"{JDBC_JAR},{NEO4J_JAR}\")\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    # === Mismo Python en driver/worker + fixes Windows ===\n",
    "    .config(\"spark.pyspark.driver.python\", PYTHON)\n",
    "    .config(\"spark.pyspark.python\", PYTHON)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", PYTHON)\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.python.use.daemon\", \"false\")\n",
    "    .config(\"spark.local.dir\", r\"C:\\spark\\tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Opcional: mejora performance\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8e5bf7-ab4d-4f0a-b39f-67dfbeb88ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_props = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"fetchsize\": str(JDBC_FETCHSIZE)\n",
    "}\n",
    "\n",
    "# Accounts\n",
    "accounts_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE1}\")\n",
    "    .option(\"partitionColumn\", \"account\")\n",
    "    .option(\"lowerBound\", 1)                  \n",
    "    .option(\"upperBound\", 2000000)\n",
    "    .option(\"numPartitions\", 8)               \n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "#Para particionado eficiente JDBC\n",
    "acc_bounds = accounts_df.select(\n",
    "    F.min(\"account\").cast(\"long\").alias(\"lo\"),\n",
    "    F.max(\"account\").cast(\"long\").alias(\"hi\")\n",
    ").first()\n",
    "acc_lo, acc_hi = int(acc_bounds[\"lo\"]), int(acc_bounds[\"hi\"])\n",
    "\n",
    "# Transferences\n",
    "tx_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE2}\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", 1)\n",
    "    .option(\"upperBound\", 9500000)\n",
    "    .option(\"numPartitions\", 8)\n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "\n",
    "# Statements\n",
    "stm_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE3}\")\n",
    "    .option(\"partitionColumn\", \"account\")       # particionamos por cuenta\n",
    "    .option(\"lowerBound\", acc_lo)\n",
    "    .option(\"upperBound\", acc_hi)\n",
    "    .option(\"numPartitions\", 8)                 # ajústalo a tu máquina/cluster\n",
    "    .options(**jdbc_props)\n",
    "    .load()\n",
    "    .select(\n",
    "        F.col(\"account\").cast(\"long\").alias(\"account\"),\n",
    "        F.col(\"date_time\").alias(\"date_time\"),\n",
    "        F.col(\"txn_id\").cast(\"long\").alias(\"txn_id\"),\n",
    "        F.col(\"direction\").alias(\"direction\"),\n",
    "        F.col(\"delta_amount\").cast(\"double\").alias(\"delta_amount\"),\n",
    "        F.col(\"running_balance\").cast(\"double\").alias(\"running_balance\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264769a7-5114-4e2f-9fbd-aa4f98bb63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- sender_account: long (nullable = true)\n",
      " |-- receiver_account: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_currency: string (nullable = true)\n",
      " |-- received_currency: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- is_laundering: integer (nullable = true)\n",
      " |-- laundering_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()\n",
    "tx_df.printSchema()\n",
    "stm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8d8e7-c092-424f-bc4c-30043f375706",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preparación de Dataframes para Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccd815c-e4b5-43c7-8fd6-6cb1fb43afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodos\n",
    "nodes_df = accounts_df.select(\n",
    "    F.col(\"account\").cast(\"long\").alias(\"account_number\"),\n",
    "    F.col(\"location\").alias(\"location\")\n",
    ").dropDuplicates([\"account_number\"])\n",
    "\n",
    "# Aristas\n",
    "edges_df = tx_df.select(\n",
    "    F.col(\"id\").cast(\"long\").alias(\"id\"),\n",
    "    F.col(\"date_time\").alias(\"timestamp\"),\n",
    "    F.col(\"sender_account\").cast(\"long\").alias(\"src\"),\n",
    "    F.col(\"receiver_account\").cast(\"long\").alias(\"dst\"),\n",
    "    F.col(\"amount\").cast(\"double\").alias(\"amount\"),\n",
    "    F.col(\"payment_currency\"),\n",
    "    F.col(\"received_currency\"),\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"is_laundering\").cast(\"int\"),\n",
    "    F.col(\"laundering_type\")\n",
    ")\n",
    "\n",
    "# masked ~ Bernoulli(0.2)\n",
    "edges_df = edges_df.withColumn(\"masked\", (F.rand(seed=42) < F.lit(0.2)).cast(\"int\"))\n",
    "\n",
    "# Opcional: particiona por destino para paralelismo estable\n",
    "edges_df = edges_df.repartition(6, \"src\")\n",
    "nodes_df = nodes_df.repartition(2, \"account_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd894e5-2907-4278-8ba9-a61af4cc99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marcadores temporales\n",
    "mov = (stm_df\n",
    "    .withColumn(\"is_credit\", (F.col(\"delta_amount\") > 0).cast(\"int\"))\n",
    "    .withColumn(\"is_debit\",  (F.col(\"delta_amount\") < 0).cast(\"int\"))\n",
    "    .withColumn(\"abs_amount\", F.abs(\"delta_amount\"))\n",
    "    .withColumn(\"ts_long\", F.col(\"date_time\").cast(\"long\"))  # para RANGE por segundos\n",
    "    .repartition(128, \"account\")                              # ajusta a tu HW/cluster\n",
    "    .persist()\n",
    ")\n",
    "_ = mov.count()  # materializa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48863019-22c2-46b7-abe4-90aa7a78b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_window_feats(df, sec):\n",
    "    # Ventana temporal deslizante por cuenta: últimos `sec` segundos, inclusiva\n",
    "    w = (Window\n",
    "         .partitionBy(\"account\")\n",
    "         .orderBy(F.col(\"ts_long\"))\n",
    "         .rangeBetween(-sec, 0))  # incluye evento actual\n",
    "\n",
    "    return (df\n",
    "        .withColumn(f\"cnt_{sec}\",       F.count(\"*\").over(w))\n",
    "        .withColumn(f\"cred_cnt_{sec}\",  F.sum(\"is_credit\").over(w))\n",
    "        .withColumn(f\"debt_cnt_{sec}\",  F.sum(\"is_debit\").over(w))\n",
    "        .withColumn(f\"cred_sum_{sec}\",  F.sum(F.when(F.col(\"is_credit\")==1, F.col(\"abs_amount\")).otherwise(0.0)).over(w))\n",
    "        .withColumn(f\"debt_sum_{sec}\",  F.sum(F.when(F.col(\"is_debit\")==1,  F.col(\"abs_amount\")).otherwise(0.0)).over(w))\n",
    "        .withColumn(f\"net_sum_{sec}\",   F.sum(\"delta_amount\").over(w))\n",
    "    )\n",
    "\n",
    "DAY = 86400\n",
    "windows_s = [7*DAY, 15*DAY, 30*DAY]\n",
    "\n",
    "mov_w = mov\n",
    "for s in windows_s:\n",
    "    mov_w = add_window_feats(mov_w, s)\n",
    "mov_w = mov_w.persist()\n",
    "_ = mov_w.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0901893-2218-4150-829b-0339da878b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos la última fila por cuenta y nos llevamos las columnas de ventanas (que ya están calculadas “hasta el último evento”).\n",
    "w_last = Window.partitionBy(\"account\").orderBy(F.col(\"date_time\").desc(), F.col(\"txn_id\").desc())\n",
    "last_rows = (mov_w\n",
    "    .withColumn(\"rn\", F.row_number().over(w_last))\n",
    "    .filter(F.col(\"rn\")==1)\n",
    ")\n",
    "\n",
    "# Selecciona y renombra columnas por ventana (más legible con sufijos w7 / w15 / w30)\n",
    "def pick_node_cols(df, sec, tag):\n",
    "    return (df.select(\n",
    "        \"account\",\n",
    "        F.col(f\"cnt_{sec}\").alias(f\"{tag}_tx_count\"),\n",
    "        F.col(f\"cred_cnt_{sec}\").alias(f\"{tag}_credit_count\"),\n",
    "        F.col(f\"debt_cnt_{sec}\").alias(f\"{tag}_debit_count\"),\n",
    "        F.col(f\"cred_sum_{sec}\").alias(f\"{tag}_credit_sum\"),\n",
    "        F.col(f\"debt_sum_{sec}\").alias(f\"{tag}_debit_sum\"),\n",
    "        F.col(f\"net_sum_{sec}\").alias(f\"{tag}_net_flow\")\n",
    "    ))\n",
    "\n",
    "node_w7   = pick_node_cols(last_rows, 7*DAY,  \"w7\")\n",
    "node_w15  = pick_node_cols(last_rows, 15*DAY, \"w15\")\n",
    "node_w30  = pick_node_cols(last_rows, 30*DAY, \"w30\")\n",
    "\n",
    "nodes_win = (node_w7\n",
    "    .join(node_w15, \"account\", \"left\")\n",
    "    .join(node_w30, \"account\", \"left\")\n",
    "    .join(\n",
    "        last_rows.select(\"account\",\n",
    "                         F.col(\"running_balance\").alias(\"current_balance\"),\n",
    "                         F.col(\"date_time\").alias(\"last_seen\")),\n",
    "        \"account\", \"left\")\n",
    ")\n",
    "\n",
    "# Enriquecer nodes_df original\n",
    "nodes_enriched_df = (nodes_df.alias(\"n\")\n",
    "    .join(nodes_win.alias(\"f\"), F.col(\"n.account_number\")==F.col(\"f.account\"), \"left\")\n",
    "    .drop(\"account\")\n",
    "    .repartition(2, \"account_number\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e12c1-ba6f-4d5a-921b-e03655bf5949",
   "metadata": {},
   "source": [
    "Features por ARISTA (contexto previo 7/15/30 para src y dst)\n",
    "\n",
    "Para cada transacción, queremos el histórico de la cuenta justo antes de esa transacción.\n",
    "\n",
    "Como la ventana que calculamos incluye el evento actual, hacemos un pequeño ajuste por dirección:\n",
    "\n",
    "Para el emisor (DEBIT): prev_cnt = cnt - 1, prev_debt_cnt = debt_cnt - 1, prev_debt_sum = debt_sum - amount, prev_net = net_sum - ( -amount ) → net_sum - delta_amount.\n",
    "\n",
    "Para el receptor (CREDIT): prev_cnt = cnt - 1, prev_cred_cnt = cred_cnt - 1, prev_cred_sum = cred_sum - amount, prev_net = net_sum - (+amount) → net_sum - delta_amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3b4ca-204f-4e03-98a9-4a1f293e8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista en el instante de cada transacción PARA EL MOVIMIENTO correspondiente\n",
    "# (i.e., una fila por txn_id para CREDIT y otra para DEBIT)\n",
    "# Sender (DEBIT)\n",
    "send_mov = (mov_w\n",
    "    .filter(F.col(\"is_debit\")==1)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"src\"),\n",
    "        F.col(\"abs_amount\").alias(\"amt\"),\n",
    "        F.col(\"delta_amount\").alias(\"delta\"),\n",
    "        *[F.col(f\"cnt_{s}\").alias(f\"s_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_cnt_{s}\").alias(f\"s_cred_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_cnt_{s}\").alias(f\"s_debt_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_sum_{s}\").alias(f\"s_cred_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_sum_{s}\").alias(f\"s_debt_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"net_sum_{s}\").alias(f\"s_net_sum_{s}\") for s in windows_s],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Receiver (CREDIT)\n",
    "recv_mov = (mov_w\n",
    "    .filter(F.col(\"is_credit\")==1)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"dst\"),\n",
    "        F.col(\"abs_amount\").alias(\"amt\"),\n",
    "        F.col(\"delta_amount\").alias(\"delta\"),\n",
    "        *[F.col(f\"cnt_{s}\").alias(f\"r_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_cnt_{s}\").alias(f\"r_cred_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_cnt_{s}\").alias(f\"r_debt_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_sum_{s}\").alias(f\"r_cred_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_sum_{s}\").alias(f\"r_debt_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"net_sum_{s}\").alias(f\"r_net_sum_{s}\") for s in windows_s],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ajuste \"previo\" (excluir el propio evento actual)\n",
    "def adjust_sender_prev(df):\n",
    "    out = df\n",
    "    for s in windows_s:\n",
    "        out = (out\n",
    "            .withColumn(f\"s_cnt_prev_{s}\",      F.col(f\"s_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"s_debt_cnt_prev_{s}\", F.col(f\"s_debt_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"s_cred_cnt_prev_{s}\", F.col(f\"s_cred_cnt_{s}\"))  # no cambia; el evento es débito\n",
    "            .withColumn(f\"s_debt_sum_prev_{s}\", F.col(f\"s_debt_sum_{s}\") - F.col(\"amt\"))\n",
    "            .withColumn(f\"s_cred_sum_prev_{s}\", F.col(f\"s_cred_sum_{s}\"))  # no cambia\n",
    "            .withColumn(f\"s_net_prev_{s}\",      F.col(f\"s_net_sum_{s}\") - F.col(\"delta\"))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def adjust_receiver_prev(df):\n",
    "    out = df\n",
    "    for s in windows_s:\n",
    "        out = (out\n",
    "            .withColumn(f\"r_cnt_prev_{s}\",      F.col(f\"r_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"r_cred_cnt_prev_{s}\", F.col(f\"r_cred_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"r_debt_cnt_prev_{s}\", F.col(f\"r_debt_cnt_{s}\"))  # no cambia; el evento es crédito\n",
    "            .withColumn(f\"r_cred_sum_prev_{s}\", F.col(f\"r_cred_sum_{s}\") - F.col(\"amt\"))\n",
    "            .withColumn(f\"r_debt_sum_prev_{s}\", F.col(f\"r_debt_sum_{s}\"))  # no cambia\n",
    "            .withColumn(f\"r_net_prev_{s}\",      F.col(f\"r_net_sum_{s}\") - F.col(\"delta\"))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "send_prev = adjust_sender_prev(send_mov).select(\n",
    "    \"id\",\"src\",\n",
    "    *[f\"s_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_cred_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_debt_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_cred_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_debt_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_net_prev_{s}\"      for s in windows_s],\n",
    ")\n",
    "\n",
    "recv_prev = adjust_receiver_prev(recv_mov).select(\n",
    "    \"id\",\"dst\",\n",
    "    *[f\"r_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_cred_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_debt_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_cred_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_debt_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_net_prev_{s}\"      for s in windows_s],\n",
    ")\n",
    "\n",
    "# Unimos a edges_df por id y checamos consistencia de src/dst\n",
    "edges_enriched = (edges_df\n",
    "    .join(send_prev, on=[\"id\",\"src\"], how=\"left\")\n",
    "    .join(recv_prev, on=[\"id\",\"dst\"], how=\"left\")\n",
    "    .repartition(6, \"src\")  # como ya lo hacías\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec281d-de42-4770-98f2-2d7c021a55a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Preparación para escritura en Neo4j + funciones helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a5ace6-703a-41bf-9f5f-8742c78991e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "\"\"\")\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6347478d-d9ea-4ea7-8b3d-36cc193459cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_BATCHSIZE = 1000\n",
    "RETRIES = 3\n",
    "SLEEP = 1.0\n",
    "\n",
    "def _get_graph():\n",
    "    # Reintentos por si la primera conexión falla\n",
    "    last = None\n",
    "    for _ in range(RETRIES):\n",
    "        try:\n",
    "            return Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "        except Exception as e:\n",
    "            last = e; time.sleep(SLEEP)\n",
    "    raise last\n",
    "\n",
    "def write_nodes_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (a:Account {account_number: row.account_number})\n",
    "        SET a.location = row.location\n",
    "        \"\"\", rows=buf)\n",
    "        buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\"account_number\": int(row[\"account_number\"]), \"location\": row[\"location\"]})\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()\n",
    "\n",
    "def write_edges_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (s:Account {account: row.src})\n",
    "    MERGE (t:Account {account: row.dst})\n",
    "    MERGE (s)-[r:TX {id: row.id}]->(t)\n",
    "    SET  r.timestamp = row.timestamp,\n",
    "         r.amount = row.amount,\n",
    "         r.payment_currency = row.payment_currency,\n",
    "         r.received_currency = row.received_currency,\n",
    "         r.payment_type = row.payment_type,\n",
    "         r.is_laundering = row.is_laundering,\n",
    "         r.laundering_type = row.laundering_type,\n",
    "         r.masked = row.masked\n",
    "    \"\"\"\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(cypher, rows=buf); buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"src\": int(row[\"src\"]),\n",
    "            \"dst\": int(row[\"dst\"]),\n",
    "            \"timestamp\": row[\"timestamp\"],\n",
    "            \"amount\": float(row[\"amount\"]) if row[\"amount\"] is not None else None,\n",
    "            \"payment_currency\": row[\"payment_currency\"],\n",
    "            \"received_currency\": row[\"received_currency\"],\n",
    "            \"payment_type\": row[\"payment_type\"],\n",
    "            \"is_laundering\": int(row[\"is_laundering\"]) if row[\"is_laundering\"] is not None else None,\n",
    "            \"laundering_type\": row[\"laundering_type\"],\n",
    "            \"masked\": int(row[\"masked\"])\n",
    "        })\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ae68d3-bb3e-4648-8c61-eea2374d21a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 7) (DESKTOP-453KJ4K executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 18 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingested = False\n",
    "\n",
    "try:\n",
    "    nodes_df.foreachPartition(write_nodes_partition)\n",
    "    edges_df.foreachPartition(write_edges_partition)\n",
    "    ingested = True\n",
    "except Exception as e:\n",
    "    #print(e)\n",
    "    print('No funca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70fb5a-31bf-44c8-9327-7e23f1d13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- StayAwake: evita suspensión en Windows ---\n",
    "import ctypes, platform, time\n",
    "\n",
    "class StayAwake:\n",
    "    \"\"\"Bloquea suspensión/apagado de pantalla mientras el contexto está activo.\"\"\"\n",
    "    ES_CONTINUOUS = 0x80000000\n",
    "    ES_SYSTEM_REQUIRED = 0x00000001\n",
    "    ES_AWAYMODE_REQUIRED = 0x00000040  # opcional: evita que entre en sleep por \"Away Mode\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(\n",
    "                self.ES_CONTINUOUS | self.ES_SYSTEM_REQUIRED | self.ES_AWAYMODE_REQUIRED\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if platform.system() == \"Windows\":\n",
    "            # Restablece al estado normal\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(self.ES_CONTINUOUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab7d25-a478-424b-9242-d7a22e52ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, os, time\n",
    "from datetime import timedelta\n",
    "\n",
    "CHK_DIR = r\"E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\Anti Money Laundering Transaction Data\\checkpoints\"   # directorio de checkpoints\n",
    "os.makedirs(CHK_DIR, exist_ok=True)\n",
    "\n",
    "def write_done(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    open(path, \"w\").close()\n",
    "\n",
    "def is_done(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def count_df(df):\n",
    "    # cuenta y devuelve int\n",
    "    return df.count()\n",
    "\n",
    "def estimate_eta(done, total, start_ts):\n",
    "    now = time.time()\n",
    "    elapsed = now - start_ts\n",
    "    rate = done / elapsed if done > 0 else 0.0\n",
    "    remaining = total - done\n",
    "    eta_s = (remaining / rate) if rate > 0 else float(\"inf\")\n",
    "    pct = (done / total * 100.0) if total else 0.0\n",
    "    return pct, timedelta(seconds=int(eta_s)), timedelta(seconds=int(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa49fe-7fd9-4142-8126-61015ec26a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_nodes_with_progress(nodes_df, step=200_000):\n",
    "    total = count_df(nodes_df)\n",
    "    print(f\"[NODOS] total={total}\")\n",
    "    start = time.time()\n",
    "    done = 0\n",
    "\n",
    "    # Calculamos rangos por account_number\n",
    "    bounds = nodes_df.select(F.min(\"account_number\"), F.max(\"account_number\")).first()\n",
    "    lo, hi = int(bounds[0]), int(bounds[1])\n",
    "    for a in range(lo, hi + 1, step):\n",
    "        b = min(a + step - 1, hi)\n",
    "        tag = os.path.join(CHK_DIR, f\"nodes_{a}_{b}._DONE\")\n",
    "        if is_done(tag):\n",
    "            # si ya está hecho, estima como completado\n",
    "            cnt = nodes_df.filter((F.col(\"account_number\") >= a) & (F.col(\"account_number\") <= b)).count()\n",
    "            done += cnt\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[NODOS] Skip {a}-{b} (hecho). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = nodes_df.filter((F.col(\"account_number\") >= a) & (F.col(\"account_number\") <= b))\n",
    "        batch_cnt = batch_df.count()\n",
    "        if batch_cnt == 0:\n",
    "            write_done(tag);  # marca vacío\n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df.write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\",\"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"labels\", \":Account\")\n",
    "            .option(\"node.keys\", \"account_number\")\n",
    "            .option(\"batch.size\", \"10000\")\n",
    "            .option(\"transaction.retries\", \"5\")\n",
    "            .option(\"transaction.retry.timeout\", \"5000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += batch_cnt\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[NODOS] {a}-{b} -> {batch_cnt} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea5975-bc31-4e07-af80-58fafc4d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_edges_with_progress(edges_df, step=500_000, writers=4, batch_size=15000):\n",
    "    total = count_df(edges_df)\n",
    "    print(f\"[RELS] total={total}\")\n",
    "    start = time.time()\n",
    "    done = 0\n",
    "\n",
    "    bounds = edges_df.select(F.min(\"id\"), F.max(\"id\")).first()\n",
    "    lo, hi = int(bounds[0]), int(bounds[1])\n",
    "\n",
    "    for a in range(lo, hi + 1, step):\n",
    "        b = min(a + step - 1, hi)\n",
    "        tag = os.path.join(CHK_DIR, f\"rels_{a}_{b}._DONE\")\n",
    "        if is_done(tag):\n",
    "            cnt = edges_df.filter((F.col(\"id\") >= a) & (F.col(\"id\") <= b)).count()\n",
    "            done += cnt\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[RELS] Skip {a}-{b} (hecho). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = edges_df.filter((F.col(\"id\") >= a) & (F.col(\"id\") <= b))\n",
    "        batch_cnt = batch_df.count()\n",
    "        if batch_cnt == 0:\n",
    "            write_done(tag); \n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers)  # controla concurrencia de writers -> bloquea candados\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\", \"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"relationship\", \"TX\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.keys\", \"id\")\n",
    "            .option(\"relationship.source.labels\", \":Account\")\n",
    "            .option(\"relationship.target.labels\", \":Account\")\n",
    "            .option(\"relationship.source.node.keys\", \"src:account_number\")\n",
    "            .option(\"relationship.target.node.keys\", \"dst:account_number\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.properties\",\n",
    "                    \"timestamp,amount,payment_currency,received_currency,\"\n",
    "                    \"payment_type,is_laundering,laundering_type,masked\")\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"5\")\n",
    "            .option(\"transaction.retry.timeout\", \"5000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += batch_cnt\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[RELS] {a}-{b} -> {batch_cnt} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25057a-6787-4057-b8b6-bd1a0c714026",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f589b6-cb7a-4e14-aaeb-6750755d97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ingested:\n",
    "    from datetime import timedelta\n",
    "    inicio = time.time()\n",
    "    with StayAwake():\n",
    "        # Nodos (una sola vez)\n",
    "        ingest_nodes_with_progress(nodes_enriched_df, step=200_000)\n",
    "\n",
    "        # Relaciones (micro-lotes)\n",
    "        ingest_edges_with_progress(edges_enriched, step=500_000, writers=4, batch_size=15000)\n",
    "\n",
    "    fin = time.time()\n",
    "    print(f\"Tiempo total: {timedelta(seconds=int(fin - inicio))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c0606-cc53-4afe-86d9-8367336ae257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "scala_version = sc._jvm.scala.util.Properties.versionNumberString()\n",
    "print(f\"Scala version: {scala_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119691a2-a848-41c7-9656-3b723cbc5305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
