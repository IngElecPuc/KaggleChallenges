{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c8e7c6d-9ca9-438c-a6cb-d130d992f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores disponibles: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from py2neo import Graph\n",
    "from datetime import timedelta\n",
    "import sys, os\n",
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(f\"CPU cores disponibles: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c380f8-3f18-4fc5-98d9-da76d31e290e",
   "metadata": {},
   "source": [
    "# Lectura de dataframes desde PostGreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f5ac2-db91-45c9-9eae-a3e0cdec90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Credenciales ===\n",
    "PG_URL  = 'jdbc:postgresql://localhost:5432/graphs'\n",
    "PG_USER = 'spark_ingest'\n",
    "PG_PASS = 'GYleZAI2pTBKJYl9W1PL'\n",
    "PG_SCHEMA = 'saml_d'\n",
    "PG_TABLE1 = 'accounts'\n",
    "PG_TABLE2 = 'transferences'\n",
    "PG_TABLE3 = 'statements'\n",
    "\n",
    "JDBC_JAR = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\postgresql-42.7.4.jar\"\n",
    "JDBC_BATCHSIZE = 10000\n",
    "JDBC_FETCHSIZE = 10000\n",
    "\n",
    "NEO4J_JAR  = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\neo4j-connector-apache-spark_2.13-5.3.11-SNAPSHOT_for_spark_3.jar\"\n",
    "NEO4J_URI  = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"Banco.69\"\n",
    "NEO4J_DDBB = \"saml-d\"\n",
    "\n",
    "PYTHON = sys.executable  # python del kernel Jupyter\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"postgres-to-neo4j-graph\")\n",
    "    .master(\"local[*]\")\n",
    "    # === JARs locales ===\n",
    "    .config(\"spark.jars\", f\"{JDBC_JAR},{NEO4J_JAR}\")\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    # === Mismo Python en driver/worker + fixes Windows ===\n",
    "    .config(\"spark.pyspark.driver.python\", PYTHON)\n",
    "    .config(\"spark.pyspark.python\", PYTHON)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", PYTHON)\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.python.use.daemon\", \"false\")\n",
    "    .config(\"spark.local.dir\", r\"C:\\spark\\tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Opcional: mejora performance\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8e5bf7-ab4d-4f0a-b39f-67dfbeb88ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_props = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"fetchsize\": str(JDBC_FETCHSIZE)\n",
    "}\n",
    "\n",
    "# Accounts\n",
    "accounts_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE1}\")\n",
    "    .option(\"partitionColumn\", \"account\")\n",
    "    .option(\"lowerBound\", 1)                  \n",
    "    .option(\"upperBound\", 2000000)\n",
    "    .option(\"numPartitions\", 16)               \n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "#Para particionado eficiente JDBC\n",
    "acc_bounds = accounts_df.select(\n",
    "    F.min(\"account\").cast(\"long\").alias(\"lo\"),\n",
    "    F.max(\"account\").cast(\"long\").alias(\"hi\")\n",
    ").first()\n",
    "acc_lo, acc_hi = int(acc_bounds[\"lo\"]), int(acc_bounds[\"hi\"])\n",
    "\n",
    "# Transferences\n",
    "tx_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE2}\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", 1)\n",
    "    .option(\"upperBound\", 9500000)\n",
    "    .option(\"numPartitions\", 64)\n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "\n",
    "# Statements\n",
    "stm_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE3}\")\n",
    "    .option(\"partitionColumn\", \"account\")       # particionamos por cuenta\n",
    "    .option(\"lowerBound\", acc_lo)\n",
    "    .option(\"upperBound\", acc_hi)\n",
    "    .option(\"numPartitions\", 64)                 # ajústalo a tu máquina/cluster\n",
    "    .options(**jdbc_props)\n",
    "    .load()\n",
    "    .select(\n",
    "        F.col(\"account\").cast(\"long\").alias(\"account\"),\n",
    "        F.col(\"date_time\").alias(\"date_time\"),\n",
    "        F.col(\"txn_id\").cast(\"long\").alias(\"txn_id\"),\n",
    "        F.col(\"direction\").alias(\"direction\"),\n",
    "        F.col(\"delta_amount\").cast(\"double\").alias(\"delta_amount\"),\n",
    "        F.col(\"running_balance\").cast(\"double\").alias(\"running_balance\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264769a7-5114-4e2f-9fbd-aa4f98bb63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- sender_account: long (nullable = true)\n",
      " |-- receiver_account: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_currency: string (nullable = true)\n",
      " |-- received_currency: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- is_laundering: integer (nullable = true)\n",
      " |-- laundering_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- account: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- txn_id: long (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- delta_amount: double (nullable = true)\n",
      " |-- running_balance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()\n",
    "tx_df.printSchema()\n",
    "stm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8d8e7-c092-424f-bc4c-30043f375706",
   "metadata": {},
   "source": [
    "# Preparación de Dataframes para Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccd815c-e4b5-43c7-8fd6-6cb1fb43afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodos\n",
    "nodes_df = accounts_df.select(\n",
    "    F.col(\"account\").cast(\"long\").alias(\"account_number\"),\n",
    "    F.col(\"location\").alias(\"location\")\n",
    ").dropDuplicates([\"account_number\"])\n",
    "\n",
    "# Aristas\n",
    "edges_df = tx_df.select(\n",
    "    F.col(\"id\").cast(\"long\").alias(\"id\"),\n",
    "    F.col(\"date_time\").alias(\"timestamp\"),\n",
    "    F.col(\"sender_account\").cast(\"long\").alias(\"src\"),\n",
    "    F.col(\"receiver_account\").cast(\"long\").alias(\"dst\"),\n",
    "    F.col(\"amount\").cast(\"double\").alias(\"amount\"),\n",
    "    F.col(\"payment_currency\"),\n",
    "    F.col(\"received_currency\"),\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"is_laundering\").cast(\"int\"),\n",
    "    F.col(\"laundering_type\")\n",
    ")\n",
    "\n",
    "# masked ~ Bernoulli(0.2)\n",
    "edges_df = edges_df.withColumn(\"masked\", (F.rand(seed=42) < F.lit(0.2)).cast(\"int\"))\n",
    "\n",
    "# Opcional: particiona por destino para paralelismo estable\n",
    "edges_df = edges_df.repartition(256, \"src\")\n",
    "nodes_df = nodes_df.repartition(64, \"account_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd894e5-2907-4278-8ba9-a61af4cc99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marcadores temporales\n",
    "mov = (stm_df\n",
    "    .withColumn(\"is_credit\", (F.col(\"delta_amount\") > 0).cast(\"int\"))\n",
    "    .withColumn(\"is_debit\",  (F.col(\"delta_amount\") < 0).cast(\"int\"))\n",
    "    .withColumn(\"abs_amount\", F.abs(\"delta_amount\"))\n",
    "    .withColumn(\"ts_long\", F.col(\"date_time\").cast(\"long\"))  # para RANGE por segundos\n",
    "    .repartition(128, \"account\")                              # ajusta a tu HW/cluster\n",
    "    .persist()\n",
    ")\n",
    "_ = mov.count()  # materializa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48863019-22c2-46b7-abe4-90aa7a78b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_window_feats(df, sec):\n",
    "    # Ventana temporal deslizante por cuenta: últimos `sec` segundos, inclusiva\n",
    "    w = (Window\n",
    "         .partitionBy(\"account\")\n",
    "         .orderBy(F.col(\"ts_long\"))\n",
    "         .rangeBetween(-sec, 0))  # incluye evento actual\n",
    "\n",
    "    return (df\n",
    "        .withColumn(f\"cnt_{sec}\",       F.count(\"*\").over(w))\n",
    "        .withColumn(f\"cred_cnt_{sec}\",  F.sum(\"is_credit\").over(w))\n",
    "        .withColumn(f\"debt_cnt_{sec}\",  F.sum(\"is_debit\").over(w))\n",
    "        .withColumn(f\"cred_sum_{sec}\",  F.sum(F.when(F.col(\"is_credit\")==1, F.col(\"abs_amount\")).otherwise(0.0)).over(w))\n",
    "        .withColumn(f\"debt_sum_{sec}\",  F.sum(F.when(F.col(\"is_debit\")==1,  F.col(\"abs_amount\")).otherwise(0.0)).over(w))\n",
    "        .withColumn(f\"net_sum_{sec}\",   F.sum(\"delta_amount\").over(w))\n",
    "    )\n",
    "\n",
    "DAY = 86400\n",
    "windows_s = [7*DAY, 15*DAY, 30*DAY]\n",
    "\n",
    "mov_w = mov\n",
    "for s in windows_s:\n",
    "    mov_w = add_window_feats(mov_w, s)\n",
    "mov_w = mov_w.persist()\n",
    "_ = mov_w.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0901893-2218-4150-829b-0339da878b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos la última fila por cuenta y nos llevamos las columnas de ventanas (que ya están calculadas “hasta el último evento”).\n",
    "w_last = Window.partitionBy(\"account\").orderBy(F.col(\"date_time\").desc(), F.col(\"txn_id\").desc())\n",
    "last_rows = (mov_w\n",
    "    .withColumn(\"rn\", F.row_number().over(w_last))\n",
    "    .filter(F.col(\"rn\")==1)\n",
    ")\n",
    "\n",
    "# Selecciona y renombra columnas por ventana (más legible con sufijos w7 / w15 / w30)\n",
    "def pick_node_cols(df, sec, tag):\n",
    "    return (df.select(\n",
    "        \"account\",\n",
    "        F.col(f\"cnt_{sec}\").alias(f\"{tag}_tx_count\"),\n",
    "        F.col(f\"cred_cnt_{sec}\").alias(f\"{tag}_credit_count\"),\n",
    "        F.col(f\"debt_cnt_{sec}\").alias(f\"{tag}_debit_count\"),\n",
    "        F.col(f\"cred_sum_{sec}\").alias(f\"{tag}_credit_sum\"),\n",
    "        F.col(f\"debt_sum_{sec}\").alias(f\"{tag}_debit_sum\"),\n",
    "        F.col(f\"net_sum_{sec}\").alias(f\"{tag}_net_flow\")\n",
    "    ))\n",
    "\n",
    "node_w7   = pick_node_cols(last_rows, 7*DAY,  \"w7\")\n",
    "node_w15  = pick_node_cols(last_rows, 15*DAY, \"w15\")\n",
    "node_w30  = pick_node_cols(last_rows, 30*DAY, \"w30\")\n",
    "\n",
    "nodes_win = (node_w7\n",
    "    .join(node_w15, \"account\", \"left\")\n",
    "    .join(node_w30, \"account\", \"left\")\n",
    "    .join(\n",
    "        last_rows.select(\"account\",\n",
    "                         F.col(\"running_balance\").alias(\"current_balance\"),\n",
    "                         F.col(\"date_time\").alias(\"last_seen\")),\n",
    "        \"account\", \"left\")\n",
    ")\n",
    "\n",
    "# Enriquecer nodes_df original\n",
    "nodes_enriched_df = (nodes_df.alias(\"n\")\n",
    "    .join(nodes_win.alias(\"f\"), F.col(\"n.account_number\")==F.col(\"f.account\"), \"left\")\n",
    "    .drop(\"account\")\n",
    "    .repartition(64, \"account_number\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e12c1-ba6f-4d5a-921b-e03655bf5949",
   "metadata": {},
   "source": [
    "Features por ARISTA (contexto previo 7/15/30 para src y dst)\n",
    "\n",
    "Para cada transacción, queremos el histórico de la cuenta justo antes de esa transacción.\n",
    "\n",
    "Como la ventana que calculamos incluye el evento actual, hacemos un pequeño ajuste por dirección:\n",
    "\n",
    "Para el emisor (DEBIT): prev_cnt = cnt - 1, prev_debt_cnt = debt_cnt - 1, prev_debt_sum = debt_sum - amount, prev_net = net_sum - ( -amount ) → net_sum - delta_amount.\n",
    "\n",
    "Para el receptor (CREDIT): prev_cnt = cnt - 1, prev_cred_cnt = cred_cnt - 1, prev_cred_sum = cred_sum - amount, prev_net = net_sum - (+amount) → net_sum - delta_amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5d3b4ca-204f-4e03-98a9-4a1f293e8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista en el instante de cada transacción PARA EL MOVIMIENTO correspondiente\n",
    "# (i.e., una fila por txn_id para CREDIT y otra para DEBIT)\n",
    "# Sender (DEBIT)\n",
    "send_mov = (mov_w\n",
    "    .filter(F.col(\"is_debit\")==1)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"src\"),\n",
    "        F.col(\"abs_amount\").alias(\"amt\"),\n",
    "        F.col(\"delta_amount\").alias(\"delta\"),\n",
    "        *[F.col(f\"cnt_{s}\").alias(f\"s_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_cnt_{s}\").alias(f\"s_cred_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_cnt_{s}\").alias(f\"s_debt_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_sum_{s}\").alias(f\"s_cred_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_sum_{s}\").alias(f\"s_debt_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"net_sum_{s}\").alias(f\"s_net_sum_{s}\") for s in windows_s],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Receiver (CREDIT)\n",
    "recv_mov = (mov_w\n",
    "    .filter(F.col(\"is_credit\")==1)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"dst\"),\n",
    "        F.col(\"abs_amount\").alias(\"amt\"),\n",
    "        F.col(\"delta_amount\").alias(\"delta\"),\n",
    "        *[F.col(f\"cnt_{s}\").alias(f\"r_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_cnt_{s}\").alias(f\"r_cred_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_cnt_{s}\").alias(f\"r_debt_cnt_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"cred_sum_{s}\").alias(f\"r_cred_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"debt_sum_{s}\").alias(f\"r_debt_sum_{s}\") for s in windows_s],\n",
    "        *[F.col(f\"net_sum_{s}\").alias(f\"r_net_sum_{s}\") for s in windows_s],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ajuste \"previo\" (excluir el propio evento actual)\n",
    "def adjust_sender_prev(df):\n",
    "    out = df\n",
    "    for s in windows_s:\n",
    "        out = (out\n",
    "            .withColumn(f\"s_cnt_prev_{s}\",      F.col(f\"s_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"s_debt_cnt_prev_{s}\", F.col(f\"s_debt_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"s_cred_cnt_prev_{s}\", F.col(f\"s_cred_cnt_{s}\"))  # no cambia; el evento es débito\n",
    "            .withColumn(f\"s_debt_sum_prev_{s}\", F.col(f\"s_debt_sum_{s}\") - F.col(\"amt\"))\n",
    "            .withColumn(f\"s_cred_sum_prev_{s}\", F.col(f\"s_cred_sum_{s}\"))  # no cambia\n",
    "            .withColumn(f\"s_net_prev_{s}\",      F.col(f\"s_net_sum_{s}\") - F.col(\"delta\"))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "def adjust_receiver_prev(df):\n",
    "    out = df\n",
    "    for s in windows_s:\n",
    "        out = (out\n",
    "            .withColumn(f\"r_cnt_prev_{s}\",      F.col(f\"r_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"r_cred_cnt_prev_{s}\", F.col(f\"r_cred_cnt_{s}\") - 1)\n",
    "            .withColumn(f\"r_debt_cnt_prev_{s}\", F.col(f\"r_debt_cnt_{s}\"))  # no cambia; el evento es crédito\n",
    "            .withColumn(f\"r_cred_sum_prev_{s}\", F.col(f\"r_cred_sum_{s}\") - F.col(\"amt\"))\n",
    "            .withColumn(f\"r_debt_sum_prev_{s}\", F.col(f\"r_debt_sum_{s}\"))  # no cambia\n",
    "            .withColumn(f\"r_net_prev_{s}\",      F.col(f\"r_net_sum_{s}\") - F.col(\"delta\"))\n",
    "        )\n",
    "    return out\n",
    "\n",
    "send_prev = adjust_sender_prev(send_mov).select(\n",
    "    \"id\",\"src\",\n",
    "    *[f\"s_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_cred_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_debt_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_cred_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_debt_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"s_net_prev_{s}\"      for s in windows_s],\n",
    ")\n",
    "\n",
    "recv_prev = adjust_receiver_prev(recv_mov).select(\n",
    "    \"id\",\"dst\",\n",
    "    *[f\"r_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_cred_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_debt_cnt_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_cred_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_debt_sum_prev_{s}\" for s in windows_s],\n",
    "    *[f\"r_net_prev_{s}\"      for s in windows_s],\n",
    ")\n",
    "\n",
    "# Unimos a edges_df por id y checamos consistencia de src/dst\n",
    "edges_enriched = (edges_df\n",
    "    .join(send_prev, on=[\"id\",\"src\"], how=\"left\")\n",
    "    .join(recv_prev, on=[\"id\",\"dst\"], how=\"left\")\n",
    "    .repartition(256, \"src\")  # como ya lo hacías\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec281d-de42-4770-98f2-2d7c021a55a5",
   "metadata": {},
   "source": [
    "# Preparación para escritura en Neo4j + funciones helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a5ace6-703a-41bf-9f5f-8742c78991e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "\"\"\")\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6347478d-d9ea-4ea7-8b3d-36cc193459cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_BATCHSIZE = 1000\n",
    "RETRIES = 3\n",
    "SLEEP = 1.0\n",
    "\n",
    "def _get_graph():\n",
    "    # Reintentos por si la primera conexión falla\n",
    "    last = None\n",
    "    for _ in range(RETRIES):\n",
    "        try:\n",
    "            return Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "        except Exception as e:\n",
    "            last = e; time.sleep(SLEEP)\n",
    "    raise last\n",
    "\n",
    "def write_nodes_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (a:Account {account_number: row.account_number})\n",
    "        SET a.location = row.location\n",
    "        \"\"\", rows=buf)\n",
    "        buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\"account_number\": int(row[\"account_number\"]), \"location\": row[\"location\"]})\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()\n",
    "\n",
    "def write_edges_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (s:Account {account: row.src})\n",
    "    MERGE (t:Account {account: row.dst})\n",
    "    MERGE (s)-[r:TX {id: row.id}]->(t)\n",
    "    SET  r.timestamp = row.timestamp,\n",
    "         r.amount = row.amount,\n",
    "         r.payment_currency = row.payment_currency,\n",
    "         r.received_currency = row.received_currency,\n",
    "         r.payment_type = row.payment_type,\n",
    "         r.is_laundering = row.is_laundering,\n",
    "         r.laundering_type = row.laundering_type,\n",
    "         r.masked = row.masked\n",
    "    \"\"\"\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(cypher, rows=buf); buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"src\": int(row[\"src\"]),\n",
    "            \"dst\": int(row[\"dst\"]),\n",
    "            \"timestamp\": row[\"timestamp\"],\n",
    "            \"amount\": float(row[\"amount\"]) if row[\"amount\"] is not None else None,\n",
    "            \"payment_currency\": row[\"payment_currency\"],\n",
    "            \"received_currency\": row[\"received_currency\"],\n",
    "            \"payment_type\": row[\"payment_type\"],\n",
    "            \"is_laundering\": int(row[\"is_laundering\"]) if row[\"is_laundering\"] is not None else None,\n",
    "            \"laundering_type\": row[\"laundering_type\"],\n",
    "            \"masked\": int(row[\"masked\"])\n",
    "        })\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c70fb5a-31bf-44c8-9327-7e23f1d13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- StayAwake: evita suspensión en Windows ---\n",
    "import ctypes, platform, time\n",
    "\n",
    "class StayAwake:\n",
    "    \"\"\"Bloquea suspensión/apagado de pantalla mientras el contexto está activo.\"\"\"\n",
    "    ES_CONTINUOUS = 0x80000000\n",
    "    ES_SYSTEM_REQUIRED = 0x00000001\n",
    "    ES_AWAYMODE_REQUIRED = 0x00000040  # opcional: evita que entre en sleep por \"Away Mode\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(\n",
    "                self.ES_CONTINUOUS | self.ES_SYSTEM_REQUIRED | self.ES_AWAYMODE_REQUIRED\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if platform.system() == \"Windows\":\n",
    "            # Restablece al estado normal\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(self.ES_CONTINUOUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ab7d25-a478-424b-9242-d7a22e52ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, os, time\n",
    "from datetime import timedelta\n",
    "\n",
    "CHK_DIR = r\"E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\Anti Money Laundering Transaction Data\\checkpoints\"   # directorio de checkpoints\n",
    "os.makedirs(CHK_DIR, exist_ok=True)\n",
    "\n",
    "def write_done(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    open(path, \"w\").close()\n",
    "\n",
    "def is_done(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def count_df(df):\n",
    "    # cuenta y devuelve int\n",
    "    return df.count()\n",
    "\n",
    "def estimate_eta(done, total, start_ts):\n",
    "    now = time.time()\n",
    "    elapsed = now - start_ts\n",
    "    rate = done / elapsed if done > 0 else 0.0\n",
    "    remaining = total - done\n",
    "    eta_s = (remaining / rate) if rate > 0 else float(\"inf\")\n",
    "    pct = (done / total * 100.0) if total else 0.0\n",
    "    return pct, timedelta(seconds=int(eta_s)), timedelta(seconds=int(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edfa49fe-7fd9-4142-8126-61015ec26a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_nodes(\n",
    "    nodes_df,\n",
    "    buckets=8,                 # 8–16 va bien para ~0.85M nodos\n",
    "    writers_per_bucket=2,      # 1–2 writers reales\n",
    "    batch_size=20000,          # 15k–25k\n",
    "    chk_prefix=\"nodes_hashbuck\"\n",
    "):\n",
    "    # 1) Bucket uniforme por hash -> lotes grandes y balanceados\n",
    "    nodes_buck = (nodes_df\n",
    "        .withColumn(\"bucket\", (F.abs(F.hash(\"account_number\")) % F.lit(buckets)))\n",
    "        .repartition(buckets, \"bucket\")           # ~1 partición por bucket\n",
    "        .persist())\n",
    "    _ = nodes_buck.count()                         # materializa una sola vez\n",
    "\n",
    "    # 2) Tamaños por bucket (para progreso) SIN contarlos en cada vuelta\n",
    "    sizes = (nodes_buck.groupBy(\"bucket\").count().collect())\n",
    "    bucket_sizes = {int(r[\"bucket\"]): int(r[\"count\"]) for r in sizes}\n",
    "    total = sum(bucket_sizes.values())\n",
    "    print(f\"[NODOS] total={total}  buckets={buckets}\")\n",
    "\n",
    "    done, start = 0, time.time()\n",
    "\n",
    "    # 3) Escribe bucket por bucket con pocos writers (evita lock contention)\n",
    "    for b in range(buckets):\n",
    "        size_b = bucket_sizes.get(b, 0)\n",
    "        tag = os.path.join(CHK_DIR, f\"{chk_prefix}_{b}._DONE\")\n",
    "\n",
    "        if size_b == 0:\n",
    "            write_done(tag);  # bucket vacío\n",
    "            continue\n",
    "        if is_done(tag):\n",
    "            done += size_b\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[NODOS] Skip bucket {b} ({size_b} filas). \"\n",
    "                  f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = nodes_buck.filter(F.col(\"bucket\")==b).drop(\"bucket\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers_per_bucket)         # 1–2 writers reales a Neo4j\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\",\"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"labels\", \":Account\")\n",
    "            .option(\"node.keys\", \"account_number\")\n",
    "            # .option(\"node.save.strategy\",\"keys\")   # si tu jar soporta esta opción\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"20\")\n",
    "            .option(\"transaction.retry.timeout\", \"60000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += size_b\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[NODOS] bucket {b} -> {size_b} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9ea5975-bc31-4e07-af80-58fafc4d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_edges(\n",
    "    edges_df,\n",
    "    buckets=16,                  # nº de buckets por hash(src) -> 8–32 suele ir bien\n",
    "    writers_per_bucket=1,        # 1 (seguro) o 2 si no ves deadlocks\n",
    "    batch_size=20000,            # 15k–25k\n",
    "    chk_prefix=\"rels_srcbuck\"    # prefijo de archivos _DONE\n",
    "):\n",
    "    # 1) Prepara buckets balanceados por hash de src\n",
    "    edges_buck = (edges_df\n",
    "        .withColumn(\"bucket\", (F.abs(F.hash(\"src\")) % F.lit(buckets)))\n",
    "        .repartition(buckets, \"bucket\")              # 1 partición aprox por bucket\n",
    "        .sortWithinPartitions(\"src\", \"id\")           # orden consistente ayuda a locks\n",
    "        .persist())\n",
    "\n",
    "    # 2) Cuenta filas por bucket UNA sola vez (evita count() repetidos)\n",
    "    counts_by_bucket = (edges_buck\n",
    "        .groupBy(\"bucket\").count()\n",
    "        .collect())\n",
    "    bucket_sizes = {int(r[\"bucket\"]): int(r[\"count\"]) for r in counts_by_bucket}\n",
    "    total = sum(bucket_sizes.values())\n",
    "    print(f\"[RELS] total={total}  buckets={buckets}\")\n",
    "\n",
    "    done = 0\n",
    "    start = time.time()\n",
    "\n",
    "    # 3) Escribe bucket por bucket con pocos writers (evita colisiones)\n",
    "    for b in range(buckets):\n",
    "        size_b = bucket_sizes.get(b, 0)\n",
    "        if size_b == 0:\n",
    "            tag = os.path.join(CHK_DIR, f\"{chk_prefix}_{b}._DONE\")\n",
    "            write_done(tag)\n",
    "            continue\n",
    "\n",
    "        tag = os.path.join(CHK_DIR, f\"{chk_prefix}_{b}._DONE\")\n",
    "        if is_done(tag):\n",
    "            done += size_b\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[RELS] Skip bucket {b} ({size_b} filas). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = edges_buck.filter(F.col(\"bucket\")==b).drop(\"bucket\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers_per_bucket)            # 1–2 writers REALES → no deadlock\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\", \"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"relationship\", \"TX\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.keys\", \"id\")\n",
    "            .option(\"relationship.source.labels\", \":Account\")\n",
    "            .option(\"relationship.target.labels\", \":Account\")\n",
    "            .option(\"relationship.source.node.keys\", \"src:account_number\")\n",
    "            .option(\"relationship.target.node.keys\", \"dst:account_number\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.properties\",\n",
    "                    \"timestamp,amount,payment_currency,received_currency,\"\n",
    "                    \"payment_type,is_laundering,laundering_type,masked\")\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"20\")      # subimos reintentos\n",
    "            .option(\"transaction.retry.timeout\", \"60000\")  # y timeout (ms)\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += size_b\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[RELS] bucket {b} -> {size_b} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25057a-6787-4057-b8b6-bd1a0c714026",
   "metadata": {},
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0f589b6-cb7a-4e14-aaeb-6750755d97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODOS] total=855460  buckets=8\n",
      "[NODOS] bucket 0 -> 106320 filas en 0:00:16. done=106320/855460 (12.43%) ETA=0:02:01 elapsed=0:00:17\n",
      "[NODOS] bucket 1 -> 107115 filas en 0:00:18. done=213435/855460 (24.95%) ETA=0:01:48 elapsed=0:00:35\n",
      "[NODOS] bucket 2 -> 106586 filas en 0:00:10. done=320021/855460 (37.41%) ETA=0:01:18 elapsed=0:00:46\n",
      "[NODOS] bucket 3 -> 107158 filas en 0:00:08. done=427179/855460 (49.94%) ETA=0:00:55 elapsed=0:00:54\n",
      "[NODOS] bucket 4 -> 106972 filas en 0:00:09. done=534151/855460 (62.44%) ETA=0:00:39 elapsed=0:01:04\n",
      "[NODOS] bucket 5 -> 107299 filas en 0:00:09. done=641450/855460 (74.98%) ETA=0:00:24 elapsed=0:01:14\n",
      "[NODOS] bucket 6 -> 106439 filas en 0:00:20. done=747889/855460 (87.43%) ETA=0:00:13 elapsed=0:01:35\n",
      "[NODOS] bucket 7 -> 107571 filas en 0:00:13. done=855460/855460 (100.00%) ETA=0:00:00 elapsed=0:01:48\n",
      "[RELS] total=9504852  buckets=16\n",
      "[RELS] bucket 0 -> 606777 filas en 0:01:24. done=606777/9504852 (6.38%) ETA=0:20:45 elapsed=0:01:24)\n",
      "[RELS] bucket 1 -> 589278 filas en 0:01:24. done=1196055/9504852 (12.58%) ETA=0:19:35 elapsed=0:02:49)\n",
      "[RELS] bucket 2 -> 598987 filas en 0:00:58. done=1795042/9504852 (18.89%) ETA=0:16:20 elapsed=0:03:48)\n",
      "[RELS] bucket 3 -> 587331 filas en 0:01:09. done=2382373/9504852 (25.06%) ETA=0:14:50 elapsed=0:04:57)\n",
      "[RELS] bucket 4 -> 602468 filas en 0:01:26. done=2984841/9504852 (31.40%) ETA=0:14:00 elapsed=0:06:24)\n",
      "[RELS] bucket 5 -> 572728 filas en 0:01:11. done=3557569/9504852 (37.43%) ETA=0:12:43 elapsed=0:07:36)\n",
      "[RELS] bucket 6 -> 571063 filas en 0:01:17. done=4128632/9504852 (43.44%) ETA=0:11:35 elapsed=0:08:54)\n",
      "[RELS] bucket 7 -> 603866 filas en 0:01:10. done=4732498/9504852 (49.79%) ETA=0:10:10 elapsed=0:10:05)\n",
      "[RELS] bucket 8 -> 587014 filas en 0:01:21. done=5319512/9504852 (55.97%) ETA=0:09:00 elapsed=0:11:26)\n",
      "[RELS] bucket 9 -> 592638 filas en 0:01:29. done=5912150/9504852 (62.20%) ETA=0:07:51 elapsed=0:12:56)\n",
      "[RELS] bucket 10 -> 602300 filas en 0:01:07. done=6514450/9504852 (68.54%) ETA=0:06:27 elapsed=0:14:04)\n",
      "[RELS] bucket 11 -> 613603 filas en 0:01:38. done=7128053/9504852 (74.99%) ETA=0:05:14 elapsed=0:15:42)\n",
      "[RELS] bucket 12 -> 599872 filas en 0:01:08. done=7727925/9504852 (81.31%) ETA=0:03:52 elapsed=0:16:50)\n",
      "[RELS] bucket 13 -> 597662 filas en 0:02:21. done=8325587/9504852 (87.59%) ETA=0:02:43 elapsed=0:19:12)\n",
      "[RELS] bucket 14 -> 586600 filas en 0:01:24. done=8912187/9504852 (93.76%) ETA=0:01:22 elapsed=0:20:37)\n",
      "[RELS] bucket 15 -> 592665 filas en 0:01:09. done=9504852/9504852 (100.00%) ETA=0:00:00 elapsed=0:21:46)\n",
      "Tiempo total: 0:30:30\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "with StayAwake():\n",
    "    # Nodos (micro-lotes)\n",
    "    ingest_nodes(nodes_enriched_df, \n",
    "                      buckets=8, \n",
    "                      writers_per_bucket=2, \n",
    "                      batch_size=20000)\n",
    "\n",
    "    # Relaciones (micro-lotes)\n",
    "    ingest_edges(edges_enriched, \n",
    "                 buckets=16, \n",
    "                 writers_per_bucket=4, \n",
    "                 batch_size=20000, \n",
    "                 chk_prefix=\"rels_srcbuck\" )\n",
    "\n",
    "fin = time.time()\n",
    "print(f\"Tiempo total: {timedelta(seconds=int(fin - inicio))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc4c0606-cc53-4afe-86d9-8367336ae257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala version: 2.13.16\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "scala_version = sc._jvm.scala.util.Properties.versionNumberString()\n",
    "print(f\"Scala version: {scala_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e77a26f2-f322-41c5-84a6-cd150cf25549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "# graph.run(\"\"\"\n",
    "# CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "# FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "# \"\"\")\n",
    "# graph.run(\"\"\"\n",
    "# CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "# FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf2671-f9cd-48a0-92ca-003d65326b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
