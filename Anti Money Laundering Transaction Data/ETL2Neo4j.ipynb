{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8e7c6d-9ca9-438c-a6cb-d130d992f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores disponibles: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from py2neo import Graph\n",
    "import sys, os\n",
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(f\"CPU cores disponibles: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f5ac2-db91-45c9-9eae-a3e0cdec90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Credenciales ===\n",
    "PG_URL  = 'jdbc:postgresql://localhost:5432/graphs'\n",
    "PG_USER = 'spark_ingest'\n",
    "PG_PASS = 'GYleZAI2pTBKJYl9W1PL'\n",
    "PG_SCHEMA = 'saml_d'\n",
    "PG_TABLE1 = 'accounts'\n",
    "PG_TABLE2 = 'transferences'\n",
    "\n",
    "JDBC_JAR = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\postgresql-42.7.4.jar\"\n",
    "JDBC_BATCHSIZE = 10000\n",
    "JDBC_FETCHSIZE = 10000\n",
    "\n",
    "NEO4J_JAR  = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\neo4j-connector-apache-spark_2.13-5.3.11-SNAPSHOT_for_spark_3.jar\"\n",
    "NEO4J_URI  = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"Banco.69\"\n",
    "NEO4J_DDBB = \"saml-d\"\n",
    "\n",
    "PYTHON = sys.executable  # python del kernel Jupyter\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"postgres-to-neo4j-graph\")\n",
    "    .master(\"local[*]\")\n",
    "    # === JARs locales ===\n",
    "    .config(\"spark.jars\", f\"{JDBC_JAR},{NEO4J_JAR}\")\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    # === Mismo Python en driver/worker + fixes Windows ===\n",
    "    .config(\"spark.pyspark.driver.python\", PYTHON)\n",
    "    .config(\"spark.pyspark.python\", PYTHON)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", PYTHON)\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.python.use.daemon\", \"false\")\n",
    "    .config(\"spark.local.dir\", r\"C:\\spark\\tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Opcional: mejora performance\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e5bf7-ab4d-4f0a-b39f-67dfbeb88ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_props = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"fetchsize\": str(JDBC_FETCHSIZE)\n",
    "}\n",
    "\n",
    "# Accounts\n",
    "accounts_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE1}\")\n",
    "    .option(\"partitionColumn\", \"account\")\n",
    "    .option(\"lowerBound\", 1)                  \n",
    "    .option(\"upperBound\", 2000000)\n",
    "    .option(\"numPartitions\", 8)               \n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "\n",
    "# Transferences\n",
    "tx_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE2}\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", 1)\n",
    "    .option(\"upperBound\", 9500000)\n",
    "    .option(\"numPartitions\", 8)\n",
    "    .options(**jdbc_props)\n",
    "    .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264769a7-5114-4e2f-9fbd-aa4f98bb63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.printSchema()\n",
    "tx_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccd815c-e4b5-43c7-8fd6-6cb1fb43afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodos\n",
    "nodes_df = accounts_df.select(\n",
    "    F.col(\"account\").cast(\"long\").alias(\"account_number\"),\n",
    "    F.col(\"location\").alias(\"location\")\n",
    ").dropDuplicates([\"account_number\"])\n",
    "\n",
    "# Aristas\n",
    "edges_df = tx_df.select(\n",
    "    F.col(\"id\").cast(\"long\").alias(\"id\"),\n",
    "    F.col(\"date_time\").alias(\"timestamp\"),\n",
    "    F.col(\"sender_account\").cast(\"long\").alias(\"src\"),\n",
    "    F.col(\"receiver_account\").cast(\"long\").alias(\"dst\"),\n",
    "    F.col(\"amount\").cast(\"double\").alias(\"amount\"),\n",
    "    F.col(\"payment_currency\"),\n",
    "    F.col(\"received_currency\"),\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"is_laundering\").cast(\"int\"),\n",
    "    F.col(\"laundering_type\")\n",
    ")\n",
    "\n",
    "# masked ~ Bernoulli(0.2)\n",
    "edges_df = edges_df.withColumn(\"masked\", (F.rand(seed=42) < F.lit(0.2)).cast(\"int\"))\n",
    "\n",
    "# Opcional: particiona por destino para paralelismo estable\n",
    "edges_df = edges_df.repartition(6, \"src\")\n",
    "nodes_df = nodes_df.repartition(2, \"account_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a5ace6-703a-41bf-9f5f-8742c78991e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "\"\"\")\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6347478d-d9ea-4ea7-8b3d-36cc193459cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_BATCHSIZE = 1000\n",
    "RETRIES = 3\n",
    "SLEEP = 1.0\n",
    "\n",
    "def _get_graph():\n",
    "    # Reintentos por si la primera conexión falla\n",
    "    last = None\n",
    "    for _ in range(RETRIES):\n",
    "        try:\n",
    "            return Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "        except Exception as e:\n",
    "            last = e; time.sleep(SLEEP)\n",
    "    raise last\n",
    "\n",
    "def write_nodes_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(\"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MERGE (a:Account {account_number: row.account_number})\n",
    "        SET a.location = row.location\n",
    "        \"\"\", rows=buf)\n",
    "        buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\"account_number\": int(row[\"account_number\"]), \"location\": row[\"location\"]})\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()\n",
    "\n",
    "def write_edges_partition(rows_iter):\n",
    "    graph = _get_graph()\n",
    "    buf = []\n",
    "    run = graph.run\n",
    "    cypher = \"\"\"\n",
    "    UNWIND $rows AS row\n",
    "    MERGE (s:Account {account: row.src})\n",
    "    MERGE (t:Account {account: row.dst})\n",
    "    MERGE (s)-[r:TX {id: row.id}]->(t)\n",
    "    SET  r.timestamp = row.timestamp,\n",
    "         r.amount = row.amount,\n",
    "         r.payment_currency = row.payment_currency,\n",
    "         r.received_currency = row.received_currency,\n",
    "         r.payment_type = row.payment_type,\n",
    "         r.is_laundering = row.is_laundering,\n",
    "         r.laundering_type = row.laundering_type,\n",
    "         r.masked = row.masked\n",
    "    \"\"\"\n",
    "    def flush():\n",
    "        if not buf: return\n",
    "        run(cypher, rows=buf); buf.clear()\n",
    "    for row in rows_iter:\n",
    "        buf.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"src\": int(row[\"src\"]),\n",
    "            \"dst\": int(row[\"dst\"]),\n",
    "            \"timestamp\": row[\"timestamp\"],\n",
    "            \"amount\": float(row[\"amount\"]) if row[\"amount\"] is not None else None,\n",
    "            \"payment_currency\": row[\"payment_currency\"],\n",
    "            \"received_currency\": row[\"received_currency\"],\n",
    "            \"payment_type\": row[\"payment_type\"],\n",
    "            \"is_laundering\": int(row[\"is_laundering\"]) if row[\"is_laundering\"] is not None else None,\n",
    "            \"laundering_type\": row[\"laundering_type\"],\n",
    "            \"masked\": int(row[\"masked\"])\n",
    "        })\n",
    "        if len(buf) >= NEO4J_BATCHSIZE: flush()\n",
    "    flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ae68d3-bb3e-4648-8c61-eea2374d21a0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 7) (DESKTOP-453KJ4K executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "Caused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\n",
      "\t... 18 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingested = False\n",
    "\n",
    "try:\n",
    "    nodes_df.foreachPartition(write_nodes_partition)\n",
    "    edges_df.foreachPartition(write_edges_partition)\n",
    "    ingested = True\n",
    "except Exception as e:\n",
    "    #print(e)\n",
    "    print('No funca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70fb5a-31bf-44c8-9327-7e23f1d13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- StayAwake: evita suspensión en Windows ---\n",
    "import ctypes, platform, time\n",
    "\n",
    "class StayAwake:\n",
    "    \"\"\"Bloquea suspensión/apagado de pantalla mientras el contexto está activo.\"\"\"\n",
    "    ES_CONTINUOUS = 0x80000000\n",
    "    ES_SYSTEM_REQUIRED = 0x00000001\n",
    "    ES_AWAYMODE_REQUIRED = 0x00000040  # opcional: evita que entre en sleep por \"Away Mode\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(\n",
    "                self.ES_CONTINUOUS | self.ES_SYSTEM_REQUIRED | self.ES_AWAYMODE_REQUIRED\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if platform.system() == \"Windows\":\n",
    "            # Restablece al estado normal\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(self.ES_CONTINUOUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab7d25-a478-424b-9242-d7a22e52ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, os, time\n",
    "from datetime import timedelta\n",
    "\n",
    "CHK_DIR = r\"E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\Anti Money Laundering Transaction Data\\checkpoints\"   # directorio de checkpoints\n",
    "os.makedirs(CHK_DIR, exist_ok=True)\n",
    "\n",
    "def write_done(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    open(path, \"w\").close()\n",
    "\n",
    "def is_done(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def count_df(df):\n",
    "    # cuenta y devuelve int\n",
    "    return df.count()\n",
    "\n",
    "def estimate_eta(done, total, start_ts):\n",
    "    now = time.time()\n",
    "    elapsed = now - start_ts\n",
    "    rate = done / elapsed if done > 0 else 0.0\n",
    "    remaining = total - done\n",
    "    eta_s = (remaining / rate) if rate > 0 else float(\"inf\")\n",
    "    pct = (done / total * 100.0) if total else 0.0\n",
    "    return pct, timedelta(seconds=int(eta_s)), timedelta(seconds=int(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa49fe-7fd9-4142-8126-61015ec26a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_nodes_with_progress(nodes_df, step=200_000):\n",
    "    total = count_df(nodes_df)\n",
    "    print(f\"[NODOS] total={total}\")\n",
    "    start = time.time()\n",
    "    done = 0\n",
    "\n",
    "    # Calculamos rangos por account_number\n",
    "    bounds = nodes_df.select(F.min(\"account_number\"), F.max(\"account_number\")).first()\n",
    "    lo, hi = int(bounds[0]), int(bounds[1])\n",
    "    for a in range(lo, hi + 1, step):\n",
    "        b = min(a + step - 1, hi)\n",
    "        tag = os.path.join(CHK_DIR, f\"nodes_{a}_{b}._DONE\")\n",
    "        if is_done(tag):\n",
    "            # si ya está hecho, estima como completado\n",
    "            cnt = nodes_df.filter((F.col(\"account_number\") >= a) & (F.col(\"account_number\") <= b)).count()\n",
    "            done += cnt\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[NODOS] Skip {a}-{b} (hecho). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = nodes_df.filter((F.col(\"account_number\") >= a) & (F.col(\"account_number\") <= b))\n",
    "        batch_cnt = batch_df.count()\n",
    "        if batch_cnt == 0:\n",
    "            write_done(tag);  # marca vacío\n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df.write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\",\"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"labels\", \":Account\")\n",
    "            .option(\"node.keys\", \"account_number\")\n",
    "            .option(\"batch.size\", \"10000\")\n",
    "            .option(\"transaction.retries\", \"5\")\n",
    "            .option(\"transaction.retry.timeout\", \"5000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += batch_cnt\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[NODOS] {a}-{b} -> {batch_cnt} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea5975-bc31-4e07-af80-58fafc4d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_edges_with_progress(edges_df, step=500_000, writers=4, batch_size=15000):\n",
    "    total = count_df(edges_df)\n",
    "    print(f\"[RELS] total={total}\")\n",
    "    start = time.time()\n",
    "    done = 0\n",
    "\n",
    "    bounds = edges_df.select(F.min(\"id\"), F.max(\"id\")).first()\n",
    "    lo, hi = int(bounds[0]), int(bounds[1])\n",
    "\n",
    "    for a in range(lo, hi + 1, step):\n",
    "        b = min(a + step - 1, hi)\n",
    "        tag = os.path.join(CHK_DIR, f\"rels_{a}_{b}._DONE\")\n",
    "        if is_done(tag):\n",
    "            cnt = edges_df.filter((F.col(\"id\") >= a) & (F.col(\"id\") <= b)).count()\n",
    "            done += cnt\n",
    "            pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[RELS] Skip {a}-{b} (hecho). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = edges_df.filter((F.col(\"id\") >= a) & (F.col(\"id\") <= b))\n",
    "        batch_cnt = batch_df.count()\n",
    "        if batch_cnt == 0:\n",
    "            write_done(tag); \n",
    "            continue\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers)  # controla concurrencia de writers -> bloquea candados\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\", \"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"relationship\", \"TX\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.keys\", \"id\")\n",
    "            .option(\"relationship.source.labels\", \":Account\")\n",
    "            .option(\"relationship.target.labels\", \":Account\")\n",
    "            .option(\"relationship.source.node.keys\", \"src:account_number\")\n",
    "            .option(\"relationship.target.node.keys\", \"dst:account_number\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.properties\",\n",
    "                    \"timestamp,amount,payment_currency,received_currency,\"\n",
    "                    \"payment_type,is_laundering,laundering_type,masked\")\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"5\")\n",
    "            .option(\"transaction.retry.timeout\", \"5000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += batch_cnt\n",
    "        write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[RELS] {a}-{b} -> {batch_cnt} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f589b6-cb7a-4e14-aaeb-6750755d97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ingested:\n",
    "    from datetime import timedelta\n",
    "    inicio = time.time()\n",
    "    with StayAwake():\n",
    "        # Nodos (una sola vez)\n",
    "        ingest_nodes_with_progress(nodes_df, step=200_000)\n",
    "\n",
    "        # Relaciones (micro-lotes)\n",
    "        ingest_edges_with_progress(edges_df, step=500_000, writers=4, batch_size=15000)\n",
    "\n",
    "    fin = time.time()\n",
    "    print(f\"Tiempo total: {timedelta(seconds=int(fin - inicio))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c0606-cc53-4afe-86d9-8367336ae257",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "scala_version = sc._jvm.scala.util.Properties.versionNumberString()\n",
    "print(f\"Scala version: {scala_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119691a2-a848-41c7-9656-3b723cbc5305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
