{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8e7c6d-9ca9-438c-a6cb-d130d992f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU cores disponibles: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from py2neo import Graph\n",
    "from datetime import timedelta\n",
    "import sys, os\n",
    "import multiprocessing\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "print(f\"CPU cores disponibles: {cpu_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c380f8-3f18-4fc5-98d9-da76d31e290e",
   "metadata": {},
   "source": [
    "# Lectura de dataframes desde PostGreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041f5ac2-db91-45c9-9eae-a3e0cdec90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Credenciales ===\n",
    "PG_URL  = 'jdbc:postgresql://localhost:5432/graphs'\n",
    "PG_USER = 'spark_ingest'\n",
    "PG_PASS = 'GYleZAI2pTBKJYl9W1PL'\n",
    "PG_SCHEMA = 'saml_d'\n",
    "PG_TABLE1 = 'accounts'\n",
    "PG_TABLE2 = 'transferences'\n",
    "PG_TABLE3 = 'statements'\n",
    "\n",
    "JDBC_JAR = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\postgresql-42.7.4.jar\"\n",
    "JDBC_BATCHSIZE = 10000\n",
    "JDBC_FETCHSIZE = 10000\n",
    "\n",
    "NEO4J_JAR  = r\"C:\\spark\\spark-4.0.1-bin-hadoop3\\jars\\neo4j-connector-apache-spark_2.13-5.3.11-SNAPSHOT_for_spark_3.jar\"\n",
    "NEO4J_URI  = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASS = \"Banco.69\"\n",
    "NEO4J_DDBB = \"saml-d\"\n",
    "\n",
    "PYTHON = sys.executable  # python del kernel Jupyter\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"postgres-to-neo4j-graph\")\n",
    "    .master(\"local[*]\")\n",
    "    # === JARs locales ===\n",
    "    .config(\"spark.jars\", f\"{JDBC_JAR},{NEO4J_JAR}\")\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{JDBC_JAR};{NEO4J_JAR}\")\n",
    "    # === Mismo Python en driver/worker + fixes Windows ===\n",
    "    .config(\"spark.pyspark.driver.python\", PYTHON)\n",
    "    .config(\"spark.pyspark.python\", PYTHON)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", PYTHON)\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.python.use.daemon\", \"false\")\n",
    "    .config(\"spark.local.dir\", r\"C:\\spark\\tmp\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"128\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Opcional: mejora performance\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8e5bf7-ab4d-4f0a-b39f-67dfbeb88ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_props = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASS,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"fetchsize\": str(JDBC_FETCHSIZE)\n",
    "}\n",
    "\n",
    "# Accounts\n",
    "accounts_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE1}\")\n",
    "    .option(\"partitionColumn\", \"account\")\n",
    "    .option(\"lowerBound\", 1)                  \n",
    "    .option(\"upperBound\", 2000000)\n",
    "    .option(\"numPartitions\", 16)               \n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "#Para particionado eficiente JDBC\n",
    "acc_bounds = accounts_df.select(\n",
    "    F.min(\"account\").cast(\"long\").alias(\"lo\"),\n",
    "    F.max(\"account\").cast(\"long\").alias(\"hi\")\n",
    ").first()\n",
    "acc_lo, acc_hi = int(acc_bounds[\"lo\"]), int(acc_bounds[\"hi\"])\n",
    "\n",
    "# Transferences\n",
    "tx_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE2}\")\n",
    "    .option(\"partitionColumn\", \"id\")\n",
    "    .option(\"lowerBound\", 1)\n",
    "    .option(\"upperBound\", 9500000)\n",
    "    .option(\"numPartitions\", 64)\n",
    "    .options(**jdbc_props)\n",
    "    .load())\n",
    "\n",
    "# Statements\n",
    "stm_df = (spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", PG_URL)\n",
    "    .option(\"dbtable\", f\"{PG_SCHEMA}.{PG_TABLE3}\")\n",
    "    .option(\"partitionColumn\", \"account\")       # particionamos por cuenta\n",
    "    .option(\"lowerBound\", acc_lo)\n",
    "    .option(\"upperBound\", acc_hi)\n",
    "    .option(\"numPartitions\", 64)                 # ajústalo a tu máquina/cluster\n",
    "    .options(**jdbc_props)\n",
    "    .load()\n",
    "    .select(\n",
    "        F.col(\"account\").cast(\"long\").alias(\"account\"),\n",
    "        F.col(\"date_time\").alias(\"date_time\"),\n",
    "        F.col(\"txn_id\").cast(\"long\").alias(\"txn_id\"),\n",
    "        F.col(\"direction\").alias(\"direction\"),\n",
    "        F.col(\"delta_amount\").cast(\"double\").alias(\"delta_amount\"),\n",
    "        F.col(\"running_balance\").cast(\"double\").alias(\"running_balance\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264769a7-5114-4e2f-9fbd-aa4f98bb63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- sender_account: long (nullable = true)\n",
      " |-- receiver_account: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_currency: string (nullable = true)\n",
      " |-- received_currency: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- is_laundering: integer (nullable = true)\n",
      " |-- laundering_type: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- account: long (nullable = true)\n",
      " |-- date_time: timestamp (nullable = true)\n",
      " |-- txn_id: long (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- delta_amount: double (nullable = true)\n",
      " |-- running_balance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accounts_df.printSchema()\n",
    "tx_df.printSchema()\n",
    "stm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8d8e7-c092-424f-bc4c-30043f375706",
   "metadata": {},
   "source": [
    "# Preparación de Dataframes para Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400dc17f-15b6-472a-9e99-b812c4fd6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodos\n",
    "nodes_df = accounts_df.select(\n",
    "    F.col(\"account\").cast(\"long\").alias(\"account_number\"),\n",
    "    F.col(\"location\").alias(\"location\")\n",
    ").dropDuplicates([\"account_number\"])\n",
    "\n",
    "# Aristas\n",
    "edges_df = tx_df.select(\n",
    "    F.col(\"id\").cast(\"long\").alias(\"id\"),\n",
    "    F.col(\"date_time\").alias(\"timestamp\"),\n",
    "    F.col(\"sender_account\").cast(\"long\").alias(\"src\"),\n",
    "    F.col(\"receiver_account\").cast(\"long\").alias(\"dst\"),\n",
    "    F.col(\"amount\").cast(\"double\").alias(\"amount\"),\n",
    "    F.col(\"payment_currency\"),\n",
    "    F.col(\"received_currency\"),\n",
    "    F.col(\"payment_type\"),\n",
    "    F.col(\"is_laundering\").cast(\"int\"),\n",
    "    F.col(\"laundering_type\")\n",
    ")\n",
    "\n",
    "# masked ~ Bernoulli(0.2)\n",
    "edges_df = edges_df.withColumn(\"masked\", (F.rand(seed=42) < F.lit(0.2)).cast(\"int\"))\n",
    "\n",
    "# Opcional: particiona por destino para paralelismo estable\n",
    "edges_df = edges_df.repartition(256, \"src\")\n",
    "nodes_df = nodes_df.repartition(64, \"account_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a8a3e7-f65d-40d6-b59b-9610f633242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) STATEMENTS sin ventanas: seq y moneda lado a lado ---\n",
    "\n",
    "# Secuencia cronológica de movimientos por cuenta (para ambos lados)\n",
    "w_seq = Window.partitionBy(\"account\").orderBy(F.col(\"date_time\").asc(), F.col(\"txn_id\").asc())\n",
    "stm_seq = (stm_df\n",
    "    .withColumn(\"seq\", F.row_number().over(w_seq))\n",
    ")\n",
    "\n",
    "# Necesitamos moneda por movimiento según lado:\n",
    "#   - si delta>0 (recibe) -> received_currency\n",
    "#   - si delta<0 (envía)  -> payment_currency\n",
    "tx_cur = (tx_df\n",
    "    .select(\n",
    "        F.col(\"id\").alias(\"txn_id\"),\n",
    "        F.col(\"payment_currency\"),\n",
    "        F.col(\"received_currency\")\n",
    "    )\n",
    ")\n",
    "\n",
    "stm_cur = (stm_seq\n",
    "    .join(tx_cur, on=\"txn_id\", how=\"left\")\n",
    "    .withColumn(\"currency\",\n",
    "        F.when(F.col(\"delta_amount\") > 0, F.col(\"received_currency\"))\n",
    "         .otherwise(F.col(\"payment_currency\"))\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c43dac57-a7f4-45cf-ad2e-6c9b6ddf962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Ledger por lado: emisor (DEBIT) y receptor (CREDIT) ---\n",
    "\n",
    "# Emisor: delta<0\n",
    "sender_ledger = (stm_cur\n",
    "    .filter(F.col(\"delta_amount\") < 0)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"src\"),\n",
    "        F.col(\"delta_amount\").alias(\"src_delta\"),\n",
    "        F.col(\"running_balance\").alias(\"src_balance_after\"),\n",
    "        (F.col(\"running_balance\") - F.col(\"delta_amount\")).alias(\"src_balance_before\"),\n",
    "        F.col(\"seq\").alias(\"src_seq\"),\n",
    "        F.col(\"currency\").alias(\"src_currency\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Receptor: delta>0\n",
    "receiver_ledger = (stm_cur\n",
    "    .filter(F.col(\"delta_amount\") > 0)\n",
    "    .select(\n",
    "        F.col(\"txn_id\").alias(\"id\"),\n",
    "        F.col(\"account\").alias(\"dst\"),\n",
    "        F.col(\"delta_amount\").alias(\"dst_delta\"),\n",
    "        F.col(\"running_balance\").alias(\"dst_balance_after\"),\n",
    "        (F.col(\"running_balance\") - F.col(\"delta_amount\")).alias(\"dst_balance_before\"),\n",
    "        F.col(\"seq\").alias(\"dst_seq\"),\n",
    "        F.col(\"currency\").alias(\"dst_currency\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3) Enriquecer aristas con ambos lados (sin ventanas) ---\n",
    "edges_enriched = (edges_df\n",
    "    .join(sender_ledger, on=[\"id\", \"src\"], how=\"left\")\n",
    "    .join(receiver_ledger, on=[\"id\", \"dst\"], how=\"left\")\n",
    "    .repartition(256, \"src\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92c0caf-34b5-4f76-b1fa-44309d87f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\venv\\lib\\site-packages\\pyspark\\sql\\classic\\column.py:359: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 4) NODOS: estado macro (first/last/current) ---\n",
    "\n",
    "# Último movimiento por cuenta\n",
    "w_last = Window.partitionBy(\"account\").orderBy(F.col(\"date_time\").desc(), F.col(\"txn_id\").desc())\n",
    "last_rows = (stm_cur\n",
    "    .withColumn(\"rn\", F.row_number().over(w_last))\n",
    "    .filter(F.col(\"rn\")==1)\n",
    "    .select(\n",
    "        F.col(\"account\"),\n",
    "        F.col(\"running_balance\").alias(\"current_balance\"),\n",
    "        F.col(\"date_time\").alias(\"last_seen\")\n",
    "    )\n",
    ")\n",
    "\n",
    "first_rows = (stm_cur\n",
    "    .groupBy(\"account\")\n",
    "    .agg(F.min(\"date_time\").alias(\"first_seen\"))\n",
    ")\n",
    "\n",
    "nodes_base = (nodes_df.alias(\"n\")\n",
    "    .join(last_rows.alias(\"lr\"), F.col(\"n.account_number\")==F.col(\"lr.account\"), \"left\")\n",
    "    .join(first_rows.alias(\"fr\"), F.col(\"n.account_number\")==F.col(\"fr.account\"), \"left\")\n",
    "    .drop(\"account\")\n",
    "    .repartition(64, \"account_number\")\n",
    ")\n",
    "\n",
    "# --- 5) Cierres mensuales por moneda ---\n",
    "\n",
    "# Marca de año/mes\n",
    "stm_monthly = (stm_cur\n",
    "    .withColumn(\"year\",  F.year(\"date_time\"))\n",
    "    .withColumn(\"month\", F.month(\"date_time\"))\n",
    ")\n",
    "\n",
    "# Último evento del mes por (account,currency,year,month)\n",
    "w_month_last = Window.partitionBy(\"account\",\"currency\",\"year\",\"month\") \\\n",
    "                     .orderBy(F.col(\"date_time\").desc(), F.col(\"txn_id\").desc())\n",
    "\n",
    "monthly_last = (stm_monthly\n",
    "    .withColumn(\"rn\", F.row_number().over(w_month_last))\n",
    "    .filter(F.col(\"rn\")==1)\n",
    "    .select(\n",
    "        \"account\",\"currency\",\"year\",\"month\",\n",
    "        F.col(\"running_balance\").alias(\"balance_close\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Pivot a columnas tipo mclose_YYYY_MM_CUR\n",
    "monthly_pivot = (monthly_last\n",
    "    .withColumn(\"prop_name\",\n",
    "        F.concat(\n",
    "            F.lit(\"mclose_\"),\n",
    "            F.format_string(\"%04d\", F.col(\"year\")),\n",
    "            F.lit(\"_\"),\n",
    "            F.format_string(\"%02d\", F.col(\"month\")),\n",
    "            F.lit(\"_\"),\n",
    "            F.col(\"currency\")\n",
    "        )\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"account\").alias(\"account_number\"),\n",
    "        \"prop_name\", \"balance_close\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compactar a un mapa (prop_name -> balance) y luego expandir al escribir\n",
    "monthly_map = (monthly_pivot\n",
    "    .groupBy(\"account_number\")\n",
    "    .agg(F.map_from_entries(F.collect_list(F.struct(\"prop_name\",\"balance_close\"))).alias(\"mclose_map\"))\n",
    ")\n",
    "\n",
    "# Unir al nodo base\n",
    "nodes_enriched_df = (nodes_base\n",
    "    .join(monthly_map, on=\"account_number\", how=\"left\")\n",
    "    # NOTA: neo4j connector no expande mapas a propiedades; podemos:\n",
    "    #  (a) expandir en Spark a columnas (si el nº de props es acotado), o\n",
    "    #  (b) escribir el mapa como JSON y expandirlo luego con APOC.\n",
    "    # Aquí te dejo (a): expandimos a columnas reales.\n",
    ")\n",
    "\n",
    "# Expandir columnas desde el mapa (versión segura para 1 año / pocas monedas):\n",
    "# 1) Listamos todas las claves existentes\n",
    "all_keys = (monthly_pivot.select(\"prop_name\").distinct().collect())\n",
    "keys = [r[\"prop_name\"] for r in all_keys]\n",
    "\n",
    "# 2) Añadimos una columna por clave\n",
    "for k in keys:\n",
    "    nodes_enriched_df = nodes_enriched_df.withColumn(k, nodes_enriched_df[\"mclose_map\"].getItem(F.lit(k)))\n",
    "\n",
    "# 3) Limpiamos el mapa\n",
    "nodes_enriched_df = nodes_enriched_df.drop(\"mclose_map\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec281d-de42-4770-98f2-2d7c021a55a5",
   "metadata": {},
   "source": [
    "# Preparación para escritura en Neo4j + funciones helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87a5ace6-703a-41bf-9f5f-8742c78991e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(No data)"
      ],
      "text/plain": [
       "(No data)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "\"\"\")\n",
    "graph.run(\"\"\"\n",
    "CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c70fb5a-31bf-44c8-9327-7e23f1d13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- StayAwake: evita suspensión en Windows ---\n",
    "import ctypes, platform, time\n",
    "\n",
    "class StayAwake:\n",
    "    \"\"\"Bloquea suspensión/apagado de pantalla mientras el contexto está activo.\"\"\"\n",
    "    ES_CONTINUOUS = 0x80000000\n",
    "    ES_SYSTEM_REQUIRED = 0x00000001\n",
    "    ES_AWAYMODE_REQUIRED = 0x00000040  # opcional: evita que entre en sleep por \"Away Mode\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        if platform.system() == \"Windows\":\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(\n",
    "                self.ES_CONTINUOUS | self.ES_SYSTEM_REQUIRED | self.ES_AWAYMODE_REQUIRED\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if platform.system() == \"Windows\":\n",
    "            # Restablece al estado normal\n",
    "            ctypes.windll.kernel32.SetThreadExecutionState(self.ES_CONTINUOUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ab7d25-a478-424b-9242-d7a22e52ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, os, time\n",
    "from datetime import timedelta\n",
    "\n",
    "CHK_DIR = r\"E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\Anti Money Laundering Transaction Data\\checkpoints\"   # directorio de checkpoints\n",
    "os.makedirs(CHK_DIR, exist_ok=True)\n",
    "\n",
    "def write_done(path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    open(path, \"w\").close()\n",
    "\n",
    "def is_done(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "def count_df(df):\n",
    "    # cuenta y devuelve int\n",
    "    return df.count()\n",
    "\n",
    "def estimate_eta(done, total, start_ts):\n",
    "    now = time.time()\n",
    "    elapsed = now - start_ts\n",
    "    rate = done / elapsed if done > 0 else 0.0\n",
    "    remaining = total - done\n",
    "    eta_s = (remaining / rate) if rate > 0 else float(\"inf\")\n",
    "    pct = (done / total * 100.0) if total else 0.0\n",
    "    return pct, timedelta(seconds=int(eta_s)), timedelta(seconds=int(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edfa49fe-7fd9-4142-8126-61015ec26a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_nodes(\n",
    "    nodes_df,\n",
    "    buckets=8,\n",
    "    writers_per_bucket=2,\n",
    "    batch_size=20000,\n",
    "    chk_prefix=\"nodes_hashbuck\"\n",
    "):\n",
    "    nodes_buck = (nodes_df\n",
    "        .withColumn(\"bucket\", (F.abs(F.hash(\"account_number\")) % F.lit(buckets)))\n",
    "        .repartition(buckets, \"bucket\")\n",
    "        .persist())\n",
    "    _ = nodes_buck.count()\n",
    "\n",
    "    sizes = (nodes_buck.groupBy(\"bucket\").count().collect())\n",
    "    bucket_sizes = {int(r[\"bucket\"]): int(r[\"count\"]) for r in sizes}\n",
    "    total = sum(bucket_sizes.values()); print(f\"[NODOS] total={total}  buckets={buckets}\")\n",
    "\n",
    "    done, start = 0, time.time()\n",
    "    for b in range(buckets):\n",
    "        size_b = bucket_sizes.get(b, 0)\n",
    "        tag = os.path.join(CHK_DIR, f\"{chk_prefix}_{b}._DONE\")\n",
    "        if size_b == 0: write_done(tag); continue\n",
    "        if is_done(tag):\n",
    "            done += size_b; pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[NODOS] Skip bucket {b} ({size_b}). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = nodes_buck.filter(F.col(\"bucket\")==b).drop(\"bucket\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers_per_bucket)\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\",\"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"labels\", \":Account\")\n",
    "            .option(\"node.keys\", \"account_number\")\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"20\")\n",
    "            .option(\"transaction.retry.timeout\", \"60000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += size_b; write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[NODOS] bucket {b} -> {size_b} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ea5975-bc31-4e07-af80-58fafc4d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_edges(\n",
    "    edges_df,\n",
    "    buckets=16,\n",
    "    writers_per_bucket=1,\n",
    "    batch_size=20000,\n",
    "    chk_prefix=\"rels_srcbuck\"\n",
    "):\n",
    "    edges_buck = (edges_df\n",
    "        .withColumn(\"bucket\", (F.abs(F.hash(\"src\")) % F.lit(buckets)))\n",
    "        .repartition(buckets, \"bucket\")\n",
    "        .sortWithinPartitions(\"src\", \"id\")\n",
    "        .persist())\n",
    "\n",
    "    counts_by_bucket = (edges_buck.groupBy(\"bucket\").count().collect())\n",
    "    bucket_sizes = {int(r[\"bucket\"]): int(r[\"count\"]) for r in counts_by_bucket}\n",
    "    total = sum(bucket_sizes.values()); print(f\"[RELS] total={total}  buckets={buckets}\")\n",
    "\n",
    "    done, start = 0, time.time()\n",
    "    for b in range(buckets):\n",
    "        size_b = bucket_sizes.get(b, 0)\n",
    "        tag = os.path.join(CHK_DIR, f\"{chk_prefix}_{b}._DONE\")\n",
    "        if size_b == 0: write_done(tag); continue\n",
    "        if is_done(tag):\n",
    "            done += size_b; pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "            print(f\"[RELS] Skip bucket {b} ({size_b}). done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed}\")\n",
    "            continue\n",
    "\n",
    "        batch_df = edges_buck.filter(F.col(\"bucket\")==b).drop(\"bucket\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        (batch_df\n",
    "            .coalesce(writers_per_bucket)\n",
    "            .write\n",
    "            .format(\"org.neo4j.spark.DataSource\")\n",
    "            .mode(\"Append\")\n",
    "            .option(\"url\", \"bolt://localhost:7687\")\n",
    "            .option(\"authentication.type\", \"basic\")\n",
    "            .option(\"authentication.basic.username\", NEO4J_USER)\n",
    "            .option(\"authentication.basic.password\", NEO4J_PASS)\n",
    "            .option(\"database\", NEO4J_DDBB)\n",
    "            .option(\"relationship\", \"TX\")\n",
    "            .option(\"relationship.save.strategy\", \"keys\")\n",
    "            .option(\"relationship.keys\", \"id\")\n",
    "            .option(\"relationship.source.labels\", \":Account\")\n",
    "            .option(\"relationship.target.labels\", \":Account\")\n",
    "            .option(\"relationship.source.node.keys\", \"src:account_number\")\n",
    "            .option(\"relationship.target.node.keys\", \"dst:account_number\")\n",
    "            .option(\"relationship.source.save.mode\", \"Match\")\n",
    "            .option(\"relationship.target.save.mode\", \"Match\")\n",
    "            .option(\"relationship.properties\",\n",
    "                    \"timestamp,amount,payment_currency,received_currency,\"\n",
    "                    \"payment_type,is_laundering,laundering_type,masked,\"\n",
    "                    \"src_delta,src_balance_before,src_balance_after,src_seq,src_currency,\"\n",
    "                    \"dst_delta,dst_balance_before,dst_balance_after,dst_seq,dst_currency\")\n",
    "            .option(\"batch.size\", str(batch_size))\n",
    "            .option(\"transaction.retries\", \"20\")\n",
    "            .option(\"transaction.retry.timeout\", \"60000\")\n",
    "            .save())\n",
    "        t1 = time.time()\n",
    "\n",
    "        done += size_b; write_done(tag)\n",
    "        pct, eta, elapsed = estimate_eta(done, total, start)\n",
    "        print(f\"[RELS] bucket {b} -> {size_b} filas en {timedelta(seconds=int(t1-t0))}. \"\n",
    "              f\"done={done}/{total} ({pct:0.2f}%) ETA={eta} elapsed={elapsed})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25057a-6787-4057-b8b6-bd1a0c714026",
   "metadata": {},
   "source": [
    "# Ingesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0f589b6-cb7a-4e14-aaeb-6750755d97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NODOS] total=855460  buckets=8\n",
      "[NODOS] bucket 0 -> 106320 filas en 0:01:52. done=106320/855460 (12.43%) ETA=0:13:12 elapsed=0:01:52)\n",
      "[NODOS] bucket 1 -> 107115 filas en 0:00:51. done=213435/855460 (24.95%) ETA=0:08:14 elapsed=0:02:44)\n",
      "[NODOS] bucket 2 -> 106586 filas en 0:00:51. done=320021/855460 (37.41%) ETA=0:06:01 elapsed=0:03:36)\n",
      "[NODOS] bucket 3 -> 107158 filas en 0:00:44. done=427179/855460 (49.94%) ETA=0:04:21 elapsed=0:04:20)\n",
      "[NODOS] bucket 4 -> 106972 filas en 0:00:52. done=534151/855460 (62.44%) ETA=0:03:08 elapsed=0:05:14)\n",
      "[NODOS] bucket 5 -> 107299 filas en 0:00:51. done=641450/855460 (74.98%) ETA=0:02:01 elapsed=0:06:05)\n",
      "[NODOS] bucket 6 -> 106439 filas en 0:00:55. done=747889/855460 (87.43%) ETA=0:01:00 elapsed=0:07:01)\n",
      "[NODOS] bucket 7 -> 107571 filas en 0:00:53. done=855460/855460 (100.00%) ETA=0:00:00 elapsed=0:07:54)\n",
      "[RELS] total=9504852  buckets=16\n",
      "[RELS] bucket 0 -> 606777 filas en 0:02:12. done=606777/9504852 (6.38%) ETA=0:32:25 elapsed=0:02:12)\n",
      "[RELS] bucket 1 -> 589278 filas en 0:01:52. done=1196055/9504852 (12.58%) ETA=0:28:21 elapsed=0:04:04)\n",
      "[RELS] bucket 2 -> 598987 filas en 0:02:01. done=1795042/9504852 (18.89%) ETA=0:26:12 elapsed=0:06:06)\n",
      "[RELS] bucket 3 -> 587331 filas en 0:01:36. done=2382373/9504852 (25.06%) ETA=0:23:03 elapsed=0:07:42)\n",
      "[RELS] bucket 4 -> 602468 filas en 0:02:19. done=2984841/9504852 (31.40%) ETA=0:21:57 elapsed=0:10:02)\n",
      "[RELS] bucket 5 -> 572728 filas en 0:02:56. done=3557569/9504852 (37.43%) ETA=0:21:42 elapsed=0:12:59)\n",
      "[RELS] bucket 6 -> 571063 filas en 0:01:32. done=4128632/9504852 (43.44%) ETA=0:18:54 elapsed=0:14:31)\n",
      "[RELS] bucket 7 -> 603866 filas en 0:01:46. done=4732498/9504852 (49.79%) ETA=0:16:25 elapsed=0:16:17)\n",
      "[RELS] bucket 8 -> 587014 filas en 0:02:09. done=5319512/9504852 (55.97%) ETA=0:14:31 elapsed=0:18:27)\n",
      "[RELS] bucket 9 -> 592638 filas en 0:02:11. done=5912150/9504852 (62.20%) ETA=0:12:33 elapsed=0:20:39)\n",
      "[RELS] bucket 10 -> 602300 filas en 0:02:04. done=6514450/9504852 (68.54%) ETA=0:10:26 elapsed=0:22:43)\n",
      "[RELS] bucket 11 -> 613603 filas en 0:02:05. done=7128053/9504852 (74.99%) ETA=0:08:16 elapsed=0:24:49)\n",
      "[RELS] bucket 12 -> 599872 filas en 0:02:02. done=7727925/9504852 (81.31%) ETA=0:06:10 elapsed=0:26:52)\n",
      "[RELS] bucket 13 -> 597662 filas en 0:02:03. done=8325587/9504852 (87.59%) ETA=0:04:05 elapsed=0:28:56)\n",
      "[RELS] bucket 14 -> 586600 filas en 0:02:56. done=8912187/9504852 (93.76%) ETA=0:02:07 elapsed=0:31:52)\n",
      "[RELS] bucket 15 -> 592665 filas en 0:02:27. done=9504852/9504852 (100.00%) ETA=0:00:00 elapsed=0:34:20)\n",
      "Tiempo total: 0:51:06\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "with StayAwake():\n",
    "    # Nodos (micro-lotes)\n",
    "    ingest_nodes(nodes_enriched_df, \n",
    "                      buckets=8, \n",
    "                      writers_per_bucket=2, \n",
    "                      batch_size=20000)\n",
    "\n",
    "    # Relaciones (micro-lotes)\n",
    "    ingest_edges(edges_enriched, \n",
    "                 buckets=16, \n",
    "                 writers_per_bucket=4, \n",
    "                 batch_size=20000, \n",
    "                 chk_prefix=\"rels_srcbuck\" )\n",
    "\n",
    "fin = time.time()\n",
    "print(f\"Tiempo total: {timedelta(seconds=int(fin - inicio))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc4c0606-cc53-4afe-86d9-8367336ae257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala version: 2.13.16\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "scala_version = sc._jvm.scala.util.Properties.versionNumberString()\n",
    "print(f\"Scala version: {scala_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e77a26f2-f322-41c5-84a6-cd150cf25549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS), name=NEO4J_DDBB)\n",
    "# graph.run(\"\"\"\n",
    "# CREATE CONSTRAINT account_unique IF NOT EXISTS\n",
    "# FOR (a:Account) REQUIRE a.account_number IS UNIQUE\n",
    "# \"\"\")\n",
    "# graph.run(\"\"\"\n",
    "# CREATE CONSTRAINT tx_unique IF NOT EXISTS\n",
    "# FOR ()-[r:TX]-() REQUIRE r.id IS UNIQUE\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf2671-f9cd-48a0-92ca-003d65326b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
