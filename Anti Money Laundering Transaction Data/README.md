# ETL Pipeline con Spark, PostgreSQL y Neo4j (Ubuntu + Windows)

Este proyecto implementa un pipeline de 3 etapas usando **Apache Spark 4.0.1**, **PostgreSQL** y **Neo4j Enterprise** para construir un grafo de transacciones a partir del dataset **SAML-D (Anti Money Laundering)**.

- **1 - ingesta_saml_d.py**  
  Ingesta el CSV en PostgreSQL (`raw.saml_d`), normaliza nombres de columnas, crea `id` secuencial y PK.

- **2 - internalETL.py**  
  Realiza un ETL dentro de PostgreSQL para generar tablas limpias:  
  `saml_d.accounts`, `saml_d.transferences`, `saml_d.statements`.

- **3 - ETL2Neo4j.py**  
  Exporta esas tablas a Neo4j mediante Spark Connector (nodos y relaciones), crea constraints si no existen.

> Los scripts soportan **Windows y Ubuntu**, seleccionan rutas autom√°ticamente y usan `--packages` (sin necesidad de JAR locales).

> Dataset original: **Kaggle ‚Äì Synthetic Transaction Monitoring Dataset (SAML-D)**  
> https://www.kaggle.com/datasets/berkanoztas/synthetic-transaction-monitoring-dataset-aml

---

## 1) Montaje de la carpeta de datos (Ubuntu)

Queremos acceder al dataset desde:

```
/mnt/datasets/Anti Money Laundering Transaction Data (SAML-D)
```

Si est√° bajo `/mnt/datasets/Datasets/...`, creamos un **bind mount**:

```bash
sudo mkdir -p "/mnt/datasets/Anti Money Laundering Transaction Data (SAML-D)"

sudo mount --bind \
  "/mnt/datasets/Datasets/Anti Money Laundering Transaction Data (SAML-D)" \
  "/mnt/datasets/Anti Money Laundering Transaction Data (SAML-D)"
```

(Opcional persistente en `/etc/fstab`):
```bash
/mnt/datasets/Datasets/Anti\040Money\040Laundering\040Transaction\040Data\040(SAML-D) \
/mnt/datasets/Anti\040Money\040Laundering\040Transaction\040Data\040(SAML-D)  none  bind  0  0
```

---

## 2) Configuraci√≥n de PostgreSQL

Ejecutar estos comandos en `psql` como usuario `postgres` para crear la base y usuario del pipeline:

```sql
CREATE USER spark_ingest WITH PASSWORD 'GYleZAI2pTBKJYl9W1PL'; --O el password que t√∫ quieras

-- Ver bases existentes
SELECT datname 
FROM pg_database
WHERE datistemplate = false;

-- Crear base si no existe
CREATE DATABASE graphs OWNER spark_ingest;

GRANT CONNECT ON DATABASE graphs TO spark_ingest;
GRANT CREATE  ON DATABASE graphs TO spark_ingest;

-- Dar acceso al esquema public
GRANT USAGE ON SCHEMA public TO spark_ingest;
GRANT CREATE ON SCHEMA public TO spark_ingest;

GRANT SELECT, INSERT, UPDATE, DELETE, REFERENCES, TRIGGER
ON ALL TABLES IN SCHEMA public TO spark_ingest;

GRANT USAGE, SELECT, UPDATE 
ON ALL SEQUENCES IN SCHEMA public TO spark_ingest;

GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO spark_ingest;

-- Que las tablas/secuencias futuras hereden permisos
ALTER DEFAULT PRIVILEGES IN SCHEMA public
GRANT SELECT, INSERT, UPDATE, DELETE, REFERENCES, TRIGGER ON TABLES TO spark_ingest;

ALTER DEFAULT PRIVILEGES IN SCHEMA public
GRANT USAGE, SELECT, UPDATE ON SEQUENCES TO spark_ingest;

ALTER ROLE spark_ingest SET search_path = raw, public;
```

---

## 3) Neo4j Enterprise ‚Äì crear o borrar base `saml-d`

```bash
# Crear base
cypher-shell -u neo4j -p 'Banco.69' -d system \
"CREATE DATABASE \`saml-d\` IF NOT EXISTS; SHOW DATABASES WHERE name = 'saml-d';"

# Borrar base
cypher-shell -u neo4j -p 'Banco.69' -d system \
"STOP DATABASE \`saml-d\`; DROP DATABASE \`saml-d\` IF EXISTS;"
```

---

## 4) Ejecuci√≥n con Spark (Ubuntu)

Los scripts ya incluyen ajustes de red (`spark.driver.host`, `bindAddress`) y rutas por OS.  
Se usan `--packages` para cargar controladores JDBC y Neo4j.

```bash
# (1) CSV -> PostgreSQL (raw.saml_d)
/opt/spark/bin/spark-submit \
  --packages org.postgresql:postgresql:42.7.4 \
  "1 - ingesta_saml_d.py"

# (2) ETL intermedio -> accounts, transferences, statements
/opt/spark/bin/spark-submit \
  --packages org.postgresql:postgresql:42.7.4 \
  "2 - internalETL.py"

# (3) PostgreSQL -> Neo4j (nodos + relaciones)
/opt/spark/bin/spark-submit \
  --packages org.postgresql:postgresql:42.7.4,org.neo4j:neo4j-connector-apache-spark_2.13:5.3.10_for_spark_3 \
  "3 - ETL2Neo4j.py"

# O bien, si son necesarias algunas configuraciones extra... (cambiar separador "," por ":" seg√∫n si se utiliza Windows o Ubuntu)

# (3.1) Con los jars en las carpetas correspondientes (chequear ETL_config.yaml) -> use_packages: false y las l√≠neas de los jars comentadas
/opt/spark/bin/spark-submit \
  --conf spark.driver.memory=8g \
  --conf spark.executor.memory=6g \
  --conf spark.memory.fraction=0.6 \  
  --conf spark.memory.storageFraction=0.3 \
  --conf spark.sql.shuffle.partitions=32 \
  --conf spark.sql.adaptive.enabled=true \  
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.task.maxFailures=1 \
  --conf spark.stage.maxConsecutiveAttempts=1 \
  --conf spark.speculation=false \
  --conf spark.ui.showConsoleProgress=false \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  --conf spark.driver.extraClassPath=/opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar \
  --conf spark.executor.extraClassPath=/opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar \
  "3 - ETL2Neo4j.py"

# (3.2) Utilizando los packages: ETL_config.yaml -> use_packages: true
/opt/spark/bin/spark-submit \
  --packages org.postgresql:postgresql:42.7.4,org.neo4j:neo4j-connector-apache-spark_2.13:5.3.3_for_spark_4 \
  --conf spark.driver.memory=8g \
  --conf spark.executor.memory=6g \
  --conf spark.memory.fraction=0.6 \
  --conf spark.memory.storageFraction=0.3 \
  --conf spark.sql.shuffle.partitions=32 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.task.maxFailures=1 \
  --conf spark.stage.maxConsecutiveAttempts=1 \
  --conf spark.speculation=false \
  --conf spark.ui.showConsoleProgress=false \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  "3 - ETL2Neo4j.py"

# (3.3) Declarando directamente el uso de los jars: ETL_config.yaml -> use_packages: false, y las l√≠neas no comentadas
/opt/spark/bin/spark-submit \
  --jars /opt/spark/jars/postgresql-42.7.4.jar,/opt/spark/jars/neo4j-connector-apache-spark_2.13-5.3.3_for_spark_4.jar \
  --conf spark.driver.memory=8g \
  --conf spark.executor.memory=6g \
  --conf spark.memory.fraction=0.6 \
  --conf spark.memory.storageFraction=0.3 \
  --conf spark.sql.shuffle.partitions=32 \
  --conf spark.sql.adaptive.enabled=true \
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.task.maxFailures=1 \
  --conf spark.stage.maxConsecutiveAttempts=1 \
  --conf spark.speculation=false \
  --conf spark.ui.showConsoleProgress=false \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  "3 - ETL2Neo4j.py"

#(3.4) Declarando indirectamente el uso de los jars: ETL_config.yaml -> use_packages: false, y las l√≠neas no comentadas
/opt/spark/bin/spark-submit \  
  --conf spark.driver.extraClassPath=/opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar \
  --conf spark.executor.extraClassPath=/opt/spark/jars/postgresql-42.7.4.jar:/opt/spark/jars/neo4j-connector-apache-spark_2.12-5.3.10_for_spark_3.jar \  
  --conf spark.driver.memory=8g \  
  --conf spark.executor.memory=6g \
  --conf spark.memory.fraction=0.6 \  
  --conf spark.memory.storageFraction=0.3 \
  --conf spark.sql.shuffle.partitions=32 \  
  --conf spark.sql.adaptive.enabled=true \  
  --conf spark.sql.adaptive.coalescePartitions.enabled=true \
  --conf spark.task.maxFailures=1 \  
  --conf spark.stage.maxConsecutiveAttempts=1 \
  --conf spark.speculation=false \
  --conf spark.ui.showConsoleProgress=false \
  --conf "spark.driver.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \
  --conf "spark.executor.extraJavaOptions=-Dlog4j2.configurationFile=config/log4j2.properties" \   
  "3 - ETL2Neo4j.py"

```

---

## 5) Helpers para iniciar/detener servicios en Ubuntu (opcional)

```bash
# Iniciar
sudo systemctl start postgresql || true
sudo systemctl start postgresql@14-main 2>/dev/null || true
sudo systemctl start postgresql@17-main 2>/dev/null || true
sudo systemctl start neo4j || true

# Detener
sudo systemctl stop neo4j || true
sudo systemctl stop postgresql || true
sudo systemctl stop postgresql@14-main 2>/dev/null || true
sudo systemctl stop postgresql@17-main 2>/dev/null || true
sudo pkill -f neo4j || true
sudo pkill -f postgres || true
```

---

## 6) üßæ Notas finales

### üîß Configuraci√≥n
- Edita rutas en `config.yaml` si cambias de equipo o disco.  
- El archivo `config.yaml` ya optimiza `spark.shuffle.partitions` y usa `MEMORY_AND_DISK` para evitar desbordes de memoria.  

### üí° Compatibilidad
- Neo4j usa **Java 21**; Spark 4.0.1 usa **Java 17** ‚Üí aseg√∫rate de que ambos est√©n correctamente configurados en tu entorno (puede requerir `update-alternatives` o exportar `JAVA_HOME` apropiado).  
- La interfaz web de Neo4j (`http://localhost:7474/browser`) no afecta la conexi√≥n `bolt://localhost:7687`.

### üì¶ Dependencias
- Verifica que los JARs declarados en `ETL_config.yaml` correspondan a **Spark 4 / Scala 2.13** (Windows) o a **Spark 3 / Scala 2.12** (Ubuntu):
  - `org.postgresql:postgresql:42.7.4`
  - `org.neo4j:neo4j-connector-apache-spark_2.13:5.3.x_for_spark_4` # O
  - `org.neo4j:neo4j-connector-apache-spark_2.13:5.3.x_for_spark_3`

---

## 7) ‚ö†Ô∏è Checkpoints del ETL (control de reintentos por bucket)

El script **`3 - ETL2Neo4j.py`** implementa un sistema de checkpoints que permite **reanudar cargas interrumpidas sin duplicar datos**.  
Cada bucket procesado genera un archivo `._DONE` dentro del directorio `.etl_checkpoints/`.

Al reejecutar el pipeline, los buckets marcados se omiten autom√°ticamente, mostrando mensajes como:


```bash
[NODOS] Skip bucket 0 (...)
[NODOS] Skip bucket 1 (...)
...
```

y no volver√° a cargar esos datos.

### üìÅ Ubicaci√≥n de los checkpoints

Por defecto (configuraci√≥n `etl.checkpoints.mode: auto` en `config.yaml`), los archivos se guardan en:

> üí° **Sugerencia:** mantener los checkpoints activados en entornos productivos y desactivarlos solo durante pruebas o depuraci√≥n.

---

### üóëÔ∏è C√≥mo borrar los checkpoints (para reprocesar todo o parte del ETL)

```bash
# Borrar todos los checkpoints (nodos y relaciones)
rm -rf .etl_checkpoints/

# Solo nodos:
rm -f .etl_checkpoints/nodes_hashbuck_*._DONE

# Solo relaciones:
rm -f .etl_checkpoints/rels_srcbuck_*._DONE
```

### üß© Propiedades de los Nodos (`:Account`)

| Propiedad | Tipo / Ejemplo | Fuente | Descripci√≥n | ¬øRecomendado? |
|------------|----------------|--------|--------------|----------------|
| **account_number** | string (`"AC12345"`) | accounts.account | Identificador √∫nico de la cuenta. | ‚úÖ Esencial (clave primaria en Neo4j). |
| **location** | string (`"US"`, `"DE"`) | accounts.location | Pa√≠s/ubicaci√≥n dominante de la cuenta. | ‚úÖ √ötil para segmentaci√≥n geogr√°fica. |
| **first_seen** | timestamp (`2023-01-02 10:00:00`) | statements.date_time (m√≠nimo) | Fecha del primer movimiento registrado. | ‚úÖ Importante para an√°lisis temporal. |
| **last_seen** | timestamp (`2024-10-05 15:45:00`) | statements.date_time (m√°ximo) | Fecha del √∫ltimo movimiento registrado. | ‚úÖ Muy √∫til. |
| **current_balance** | decimal(18,2) | statements.running_balance del √∫ltimo registro | Saldo actual estimado seg√∫n los movimientos. | ‚úÖ Central para nodos financieros. |
| **mclose_YYYY_MM_CUR** | decimal(18,2) | statements (√∫ltimo del mes) | Cierre mensual por moneda. | ‚öôÔ∏è Opcional ‚Äî bueno si haces an√°lisis hist√≥rico o visualizaci√≥n temporal. |
| **num_tx** | int | COUNT(statements) | Cantidad total de movimientos de la cuenta. | ‚öôÔ∏è Recomendado (barato de calcular, muy √∫til). |
| **total_in / total_out** | decimal | SUM(delta>0) / SUM(delta<0) | Montos totales recibidos y enviados. | ‚öôÔ∏è Opcional ‚Äî simplifica consultas de flujo. |

### üîó Propiedades de las Aristas (`:TX`)

| Propiedad | Tipo / Ejemplo | Fuente | Descripci√≥n | ¬øRecomendado? |
|------------|----------------|--------|--------------|----------------|
| **txn_id** | string | transfers.id | Identificador √∫nico de la transacci√≥n. | ‚úÖ Esencial (PK de la relaci√≥n). |
| **date_time** | timestamp | transfers.date_time | Momento en que ocurri√≥ la transacci√≥n. | ‚úÖ Fundamental para an√°lisis temporal. |
| **amount** | decimal(18,2) | transfers.amount | Monto transferido (positivo absoluto). | ‚úÖ Central. |
| **currency** | string | derivado de payment_currency / received_currency | Moneda de la transacci√≥n. | ‚úÖ Importante para balances multimoneda. |
| **src_balance_before** | decimal | statements.running_balance - delta | Saldo del emisor antes del movimiento. | ‚öôÔ∏è √ötil si necesitas trazabilidad de saldos. |
| **src_balance_after** | decimal | statements.running_balance | Saldo del emisor tras la transacci√≥n. | ‚öôÔ∏è Igual que arriba, opcional. |
| **dst_balance_before** | decimal | idem receptor | Saldo del receptor antes de recibir. | ‚öôÔ∏è Igual. |
| **dst_balance_after** | decimal | idem receptor | Saldo del receptor despu√©s de recibir. | ‚öôÔ∏è Igual. |
| **src_seq / dst_seq** | int | row_number() por cuenta | Orden secuencial del movimiento en cada cuenta. | ‚öôÔ∏è Opcional ‚Äî √∫til si consultas trayectorias temporales. |
| **masked** | boolean | Bernoulli(0.2) aleatorio | Marcador de anonimizaci√≥n o muestreo. | ‚ùå Probablemente experimental; puedes omitirlo. |

1 - ingesta_saml_d.py

Ingresa los datos del CSV original del dataset Kaggle a PostgreSQL.

Usa Spark con JDBC, crea una tabla ‚Äúraw‚Äù y normaliza columnas.

2Ô∏è‚É£ 2 - internalETL.py

Lee la tabla ‚Äúraw‚Äù y genera tres tablas limpias y derivadas:

accounts

transferences

statements

A√±ade llaves primarias y relaciones consistentes.

Escribe los resultados de nuevo a PostgreSQL (en schema saml_d).

3Ô∏è‚É£ 3 - ETL2Neo4j.py

Fase final del pipeline: Postgres ‚Üí Neo4j.

Crea sesi√≥n Spark con configuraciones del m√≥dulo spark_session.

Usa m√≥dulos del paquete ETL_pg2neo4j para:

Leer datos de Postgres (io_pg.py)

Transformar a nodos/aristas (transform.py)

Ingestar en Neo4j (io_neo4j.py)

Registrar logs y checkpoints (logs.py, utils.py)

Controlar paralelismo y recursos (diagnostics.py)

Es un orquestador: ejecuta todo el flujo end-to-end.

## üß† Estructura modular del paquete `ETL_pg2neo4j`

| üß© M√≥dulo | üìù Funci√≥n principal |
|-----------|----------------------|
| `load_config.py` | üîß Configuraci√≥n y resoluci√≥n de rutas. |
| `spark_session.py` | üöÄ Inicializaci√≥n de Spark con jars/paquetes. |
| `io_pg.py` | üíæ Lectura particionada desde Postgres. |
| `transform.py` | üîÑ Construcci√≥n de nodos y relaciones enriquecidas. |
| `io_neo4j.py` | üï∏Ô∏è Ingesta a Neo4j con batching y checkpoints. |
| `diagnostics.py` | üß† Ajustes de rendimiento y planificaci√≥n. |
| `logs.py` | üìú Gesti√≥n estructurada de logs (Spark/Python). |
| `utils.py` | üß∞ Utilidades varias (checkpoints, ETA, StayAwake). |

