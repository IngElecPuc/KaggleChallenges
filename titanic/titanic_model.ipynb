{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6234986e-88dd-44bc-b55d-8c3ec1b264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from category_encoders import HashingEncoder, TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, RobustScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from prettytable import PrettyTable\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"This Pipeline instance is not fitted yet.*\",\n",
    "    category=FutureWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda47641-119f-4a62-bdce-444b862540e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'E:/Datasets/titanic/wrangled dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732d8d34-38db-480e-89e0-0471b3e95c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w = pd.read_csv(f'{dataset_path}/train.csv')\n",
    "test_w = pd.read_csv(f'{dataset_path}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0d2dcf-6569-4a80-9f30-91e2fc23822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Family', 'Title',\n",
      "       'Deck', 'TicketPrefix'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_w.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a63ed40-9baf-4432-b2ae-f0b90b9b543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_w.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Survived'], axis=1)\n",
    "#X_test = test_w.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "y = train_w['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8a66de-aa92-4415-aa72-8a99923be4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d159b5d1-2d9b-4cb5-935f-7fa47a64e26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "      <th>Title</th>\n",
       "      <th>Deck</th>\n",
       "      <th>TicketPrefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>u</td>\n",
       "      <td>A/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>C</td>\n",
       "      <td>PC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>Miss</td>\n",
       "      <td>u</td>\n",
       "      <td>STON/O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>C</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>u</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  SibSp  Parch     Fare Embarked  Family Title Deck  \\\n",
       "0       3    0  22.0      1      0   7.2500        S       1    Mr    u   \n",
       "1       1    1  38.0      1      0  71.2833        C       1   Mrs    C   \n",
       "2       3    1  26.0      0      0   7.9250        S       0  Miss    u   \n",
       "3       1    1  35.0      1      0  53.1000        S       1   Mrs    C   \n",
       "4       3    0  35.0      0      0   8.0500        S       0    Mr    u   \n",
       "\n",
       "  TicketPrefix  \n",
       "0           A/  \n",
       "1           PC  \n",
       "2       STON/O  \n",
       "3         NONE  \n",
       "4         NONE  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42877fa-9c51-4c33-b0db-e727b3acfc8b",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74efd9f-ac9b-4424-86f6-4b79f9bf6a22",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cef45-f88b-45fa-8aa9-68296d153d5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68cb428e-e6a0-4ddc-a1f3-5c8c4bda0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'C' : 1,\n",
    "    'penalty' : 'l2',\n",
    "    'solver' : 'lbfgs',\n",
    "    'l1_ratio' : None,\n",
    "    'class_weight' : None\n",
    "}\n",
    "\n",
    "def logistic_regression_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        C=params['C'],\n",
    "        penalty=params['penalty'],\n",
    "        solver=params['solver'],\n",
    "        l1_ratio=params['l1_ratio'],\n",
    "        class_weight=params['class_weight'],\n",
    "        random_state=107 \n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132ee3ef-b43c-4f98-bb05-39659668bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the search space to avoid inconsistencies\n",
    "param_search_space = [\n",
    "    #Liblinear only supports 'l1' and 'l2' penalties\n",
    "    {\n",
    "        'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "        'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "        'model__C': loguniform(1e-4, 1e4),\n",
    "        'model__penalty': ['l1'],\n",
    "        'model__solver': ['liblinear'],\n",
    "        'model__l1_ratio': uniform(0, 1),\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }, \n",
    "    {\n",
    "        'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "        'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "        'model__C': loguniform(1e-4, 1e4),\n",
    "        'model__penalty': ['l2'],\n",
    "        'model__solver': ['liblinear'],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    },    \n",
    "    #The rest of solvers doesn't support 'l1'\n",
    "    {\n",
    "        'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "        'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "        'model__C': loguniform(1e-4, 1e4),\n",
    "        'model__penalty': ['l2'],\n",
    "        'model__solver': ['lbfgs', 'newton-cg', 'newton-cholesky'],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b87f6e-56b7-4146-934c-955a4785462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['LogisticRegression'] = {}\n",
    "models['LogisticRegression']['pipeline'] = logistic_regression_pipeline\n",
    "models['LogisticRegression']['hyperparams'] = hyperparams\n",
    "models['LogisticRegression']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a825c-a23c-45b8-91e9-9cb0bde1a726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RidgeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43fb3272-7fcb-4ac9-a185-e04e4dbbe3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'alpha' : 1,\n",
    "    'solver' : 'auto',\n",
    "    'fit_intercept' : True,\n",
    "    'class_weight' : None,\n",
    "}\n",
    "\n",
    "def ridge_classifier_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = RidgeClassifier(\n",
    "        alpha=params['alpha'],\n",
    "        solver=params['solver'],\n",
    "        fit_intercept=params['fit_intercept'],\n",
    "        class_weight=params['class_weight'],\n",
    "        random_state=107 \n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ebf436-85f0-47f5-9a70-6f955acd17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__alpha': loguniform(1e-4, 1e4), \n",
    "    'model__solver': ['auto', 'lsqr', 'sparse_cg', 'sag'],\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99ec0a9e-6b06-4b39-948b-30125e28d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['RidgeClassifier'] = {}\n",
    "models['RidgeClassifier']['pipeline'] = ridge_classifier_pipeline\n",
    "models['RidgeClassifier']['hyperparams'] = hyperparams\n",
    "models['RidgeClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ce412-cb5d-48d2-bfe2-2e2a10ab95f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75cab109-b958-4952-b0dd-a4b3fe585a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'C' : 1,\n",
    "    'fit_intercept' : True,\n",
    "    'loss' : 'hinge',\n",
    "    'average' : False,\n",
    "    'class_weight' : None\n",
    "}\n",
    "\n",
    "def passive_aggressive_classifier_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = PassiveAggressiveClassifier(\n",
    "        C=params['C'],\n",
    "        fit_intercept=params['fit_intercept'],\n",
    "        loss=params['loss'],\n",
    "        class_weight=params['class_weight'],\n",
    "        random_state=107 \n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947690e9-fb92-41d7-922f-6fed47fba056",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__C': loguniform(1e-4, 1e4),\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__loss': ['hinge', 'squared_hinge'],\n",
    "    'model__class_weight': [None, 'balanced'],\n",
    "    'model__average': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "800bcfa4-8076-4c92-8065-cc201543b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['PassiveAggressiveClassifier'] = {}\n",
    "models['PassiveAggressiveClassifier']['pipeline'] = passive_aggressive_classifier_pipeline\n",
    "models['PassiveAggressiveClassifier']['hyperparams'] = hyperparams\n",
    "models['PassiveAggressiveClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81763aa-3ffd-4129-9721-58dd6641cbad",
   "metadata": {},
   "source": [
    "### SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b0d8243-83b1-43a1-aa35-b9b9340d3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'loss' : 'hinge',\n",
    "    'penalty' : 'l2',\n",
    "    'alpha' : 0.0001,\n",
    "    'l1_ratio' : 0.15,\n",
    "    'learning_rate' : 'optimal',\n",
    "    'eta0' : 0,\n",
    "    'power_t' : 0.5,\n",
    "    'average' : False,\n",
    "    'class_weight' : None,\n",
    "}\n",
    "\n",
    "def sgd_classifier_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = SGDClassifier(\n",
    "        loss=params['loss'],\n",
    "        penalty=params['penalty'],\n",
    "        alpha=params['alpha'],\n",
    "        l1_ratio=params['l1_ratio'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        eta0=params['eta0'],\n",
    "        power_t=params['power_t'],\n",
    "        average=params['average'],\n",
    "        class_weight=params['class_weight'],\n",
    "        random_state=107 \n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a97354d5-a941-4dfc-883a-bd59ea0e4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__loss':            ['hinge', 'log_loss', 'modified_huber', 'squared_hinge'], \n",
    "    'model__penalty':         ['l2', 'l1', 'elasticnet'], \n",
    "    'model__alpha':           loguniform(1e-6, 1e-1), \n",
    "    'model__l1_ratio':        uniform(0.0, 1.0), \n",
    "    'model__learning_rate':   ['optimal', 'invscaling', 'adaptive'], \n",
    "    'model__eta0':            loguniform(1e-4, 1e-1), \n",
    "    'model__power_t':         uniform(0.1, 0.9), \n",
    "    'model__class_weight':    [None, 'balanced'], \n",
    "    'model__average':         [True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d5ecc2-075a-428e-8226-5019505d0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['SGDClassifier'] = {}\n",
    "models['SGDClassifier']['pipeline'] = sgd_classifier_pipeline\n",
    "models['SGDClassifier']['hyperparams'] = hyperparams\n",
    "models['SGDClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be934a6d-866f-44ae-8801-e4304743ad94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45a1c56e-1371-4719-a375-a63e126c3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'penalty' : None,\n",
    "    'alpha' : 0.0001,\n",
    "    'l1_ratio' : 0.15,\n",
    "    'fit_intercept' : True,\n",
    "    'eta0' : 1.0,\n",
    "    'shuffle' : True,\n",
    "    'class_weight' : None,\n",
    "}\n",
    "\n",
    "def perceptron_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = Perceptron(\n",
    "        penalty=params['penalty'],\n",
    "        alpha=params['alpha'],\n",
    "        l1_ratio=params['l1_ratio'],\n",
    "        fit_intercept=params['fit_intercept'],\n",
    "        eta0=params['eta0'],\n",
    "        shuffle=params['shuffle'],\n",
    "        class_weight=params['class_weight'],\n",
    "        random_state=107 \n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e654bf70-1b6c-432d-9eb6-98874403d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__penalty': [None, 'l2', 'l1', 'elasticnet'], \n",
    "    'model__alpha': loguniform(1e-6, 1e-1), \n",
    "    'model__l1_ratio': uniform(0.0, 1.0), \n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__eta0': loguniform(1e-4, 1e-1), \n",
    "    'model__shuffle': [True, False],\n",
    "    'model__class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "009b8a5e-cee9-4beb-97ff-4f9ed0147d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Perceptron'] = {}\n",
    "models['Perceptron']['pipeline'] = perceptron_pipeline\n",
    "models['Perceptron']['hyperparams'] = hyperparams\n",
    "models['Perceptron']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa373bc-2f0b-49e7-8c72-08f3202927d2",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f556f2d-fca6-4284-8f47-46f87e4cd5a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "369e7acd-555c-4160-98c3-6cd5e472b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'n_neighbors' : 5,\n",
    "    'weights' : 'uniform',\n",
    "    'p' : 2,\n",
    "    'leaf_size' : 30,\n",
    "}\n",
    "\n",
    "def knn_classifier_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=params['n_neighbors'],\n",
    "        weights=params['weights'],\n",
    "        p=params['p'],\n",
    "        leaf_size=params['leaf_size']\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8febe34-40e6-4a42-b13d-60f06c938cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__n_neighbors': randint(3, 51),\n",
    "    'model__weights': ['uniform', 'distance'], \n",
    "    'model__p': [1, 2], \n",
    "    'model__leaf_size': randint(10, 61), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "690607fd-10f8-412f-a36f-7079951e6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['KNeighborsClassifier'] = {}\n",
    "models['KNeighborsClassifier']['pipeline'] = knn_classifier_pipeline\n",
    "models['KNeighborsClassifier']['hyperparams'] = hyperparams\n",
    "models['KNeighborsClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2fcf4-152f-41cb-b333-5bbd1ca9a78a",
   "metadata": {},
   "source": [
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc64c6e-73f3-4935-803d-5649076052e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20fd123-2f64-4ed9-b89e-2caa61193f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'criterion' : 'gini',\n",
    "    'max_depth' : None,\n",
    "    'min_samples_split' : 2,\n",
    "    'min_samples_leaf' : 1,\n",
    "    'max_features' : None,\n",
    "    'class_weight' : None,\n",
    "    'ccp_alpha' : 0\n",
    "}\n",
    "\n",
    "def decision_tree_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') #Replaces Hashing encoder\n",
    "    #hashing_tf = HashingEncoder(n_components=params['n_components'], cols=hash_cols)\n",
    "    \n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=params['criterion'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_samples_split=params['min_samples_split'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        max_features=params['max_features'], \n",
    "        class_weight=params['class_weight'],\n",
    "        ccp_alpha=params['ccp_alpha'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd25481e-5a89-4298-b52e-382303977a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'model__max_depth': [None] + list(range(1, 21)),\n",
    "    'model__min_samples_split': randint(2, 21),\n",
    "    'model__min_samples_leaf': randint(1, 21),\n",
    "    'model__max_features': [None, 'sqrt', 'log2'],\n",
    "    'model__class_weight': [None, 'balanced'],\n",
    "    'model__ccp_alpha': loguniform(1e-4, 1e-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49c83202-fdc6-464a-95ce-e80771ee6c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['DecisionTreeClassifier'] = {}\n",
    "models['DecisionTreeClassifier']['pipeline'] = decision_tree_pipeline\n",
    "models['DecisionTreeClassifier']['hyperparams'] = hyperparams\n",
    "models['DecisionTreeClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ae53c-5d36-423e-9bb0-17763aa7e5bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d15f8-b84c-4029-975d-a154b33b76a7",
   "metadata": {},
   "source": [
    "This model is not affected too much by scaling, so I am going to leave untouch the numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4642633-a89b-4356-bf0f-bb63f1c699e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'n_estimators' : 100,\n",
    "    'criterion' : 'gini',\n",
    "    'max_depth' : None,\n",
    "    'min_samples_split' : 2,\n",
    "    'min_samples_leaf' : 5,\n",
    "    'max_features' : 'sqrt',\n",
    "    'bootstrap' : True,\n",
    "    'class_weight' : None,\n",
    "    'ccp_alpha' : 0\n",
    "}\n",
    "\n",
    "def random_forest_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') #Replaces Hashing encoder\n",
    "    #hashing_tf = HashingEncoder(n_components=params['n_components'], cols=hash_cols)\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        criterion=params['criterion'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_samples_split=params['min_samples_split'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        max_features=params['max_features'], \n",
    "        bootstrap=params['bootstrap'], \n",
    "        class_weight=params['class_weight'],\n",
    "        ccp_alpha=params['ccp_alpha'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08a7f364-91f2-4e34-92c3-ee1355609771",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__n_estimators': randint(100, 501),\n",
    "    'model__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'model__max_depth': [None] + list(range(5, 51, 5)),\n",
    "    'model__min_samples_split': randint(2, 21),\n",
    "    'model__min_samples_leaf': randint(1, 11),\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', 0.2, 0.5],\n",
    "    'model__bootstrap': [True, False],\n",
    "    'model__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "    'model__ccp_alpha': uniform(0.0, 0.01)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94caabaf-7974-4875-af99-9ff4007e2d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['RandomForestClassifier'] = {}\n",
    "models['RandomForestClassifier']['pipeline'] = random_forest_pipeline\n",
    "models['RandomForestClassifier']['hyperparams'] = hyperparams\n",
    "models['RandomForestClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee593c8a-b1b8-4193-8f5b-70ddcb2c982a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7cb2cd2-50d9-431b-9de4-86878b96d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'n_estimators' : 100,\n",
    "    'criterion' : 'gini',\n",
    "    'max_depth' : None,\n",
    "    'min_samples_split' : 2,\n",
    "    'min_samples_leaf' : 5,\n",
    "    'max_features' : 'sqrt',\n",
    "    'bootstrap' : False,\n",
    "    'class_weight' : None,\n",
    "    'ccp_alpha' : 0\n",
    "}\n",
    "\n",
    "def extra_trees_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') #Replaces Hashing encoder\n",
    "    #hashing_tf = HashingEncoder(n_components=params['n_components'], cols=hash_cols)\n",
    "    \n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        criterion=params['criterion'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_samples_split=params['min_samples_split'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        max_features=params['max_features'], \n",
    "        bootstrap=params['bootstrap'], \n",
    "        class_weight=params['class_weight'],\n",
    "        ccp_alpha=params['ccp_alpha'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f2ccff-59d3-4038-80a4-e4dd9e5a2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__n_estimators': randint(100, 501),\n",
    "    'model__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'model__max_depth': [None] + list(range(1, 31)),\n",
    "    'model__min_samples_split': randint(2, 21),\n",
    "    'model__min_samples_leaf': randint(1, 21),\n",
    "    'model__max_features': ['sqrt', 'log2', 0.2, 0.5],\n",
    "    'model__bootstrap': [True, False],\n",
    "    'model__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "    'model__ccp_alpha': uniform(0.0, 0.01)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "762040a3-2692-4f68-84bd-5be06226b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['ExtraTreesClassifier'] = {}\n",
    "models['ExtraTreesClassifier']['pipeline'] = extra_trees_pipeline\n",
    "models['ExtraTreesClassifier']['hyperparams'] = hyperparams\n",
    "models['ExtraTreesClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65a8bc-3926-485f-ac15-02a0c44ccf75",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a98a8-c4ad-4ae7-850c-d3e1a4109ab3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7598a2cc-ffd8-4103-9c75-10251fcf049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'loss': 'log_loss',\n",
    "    'criterion' : 'gini',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators' : 100,\n",
    "    'subsample' : 1,\n",
    "    'max_depth' : None,\n",
    "    'min_samples_split' : 2,\n",
    "    'min_samples_leaf' : 5,\n",
    "    'max_features' : 'sqrt'\n",
    "}\n",
    "\n",
    "def gradient_boosting_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') #Replaces Hashing encoder\n",
    "    #hashing_tf = HashingEncoder(n_components=params['n_components'], cols=hash_cols)\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        loss=params['loss'], \n",
    "        criterion=params['criterion'], \n",
    "        learning_rate=params['learning_rate'], \n",
    "        subsample=params['subsample'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_samples_split=params['min_samples_split'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        max_features=params['max_features'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([('to_dict', dict_tf), ('hasher', hasher)]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "701a8f80-9af3-4f82-b3cd-29fde186ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__loss': ['log_loss', 'exponential'],\n",
    "    'model__criterion': ['friedman_mse', 'squared_error'],\n",
    "    'model__learning_rate': loguniform(1e-3, 1e-1),\n",
    "    'model__n_estimators': randint(100, 1000),\n",
    "    'model__subsample': uniform(0.5, 0.5),\n",
    "    'model__max_depth': [None] + list(range(3, 11)),\n",
    "    'model__min_samples_split': randint(2, 20),\n",
    "    'model__min_samples_leaf': randint(1, 20),\n",
    "    'model__max_features': ['sqrt', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28870a40-46da-411c-a182-7a04cfac58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['GradientBoostingClassifier'] = {}\n",
    "models['GradientBoostingClassifier']['pipeline'] = gradient_boosting_pipeline\n",
    "models['GradientBoostingClassifier']['hyperparams'] = hyperparams\n",
    "models['GradientBoostingClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d922c3-067f-467a-a59d-c31eeccbc087",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77401320-f020-4357-ae4d-c54681a81d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "l2_regularization\n",
      "max_iter\n",
      "max_leaf_nodes\n",
      "max_depth\n",
      "min_samples_split\n",
      "min_samples_leaf\n",
      "max_features\n"
     ]
    }
   ],
   "source": [
    "for key in param_search_space.keys():\n",
    "    if 'model__' in key:\n",
    "        print(key.replace('model__', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe1017ca-8c80-4308-95f1-dc2a5e861ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'learning_rate': 1,\n",
    "    'l2_regularization' : 0,\n",
    "    'max_iter' : 100,\n",
    "    'max_depth' : None,\n",
    "    'min_samples_leaf' : 20,\n",
    "    'max_features' : 1\n",
    "}\n",
    "\n",
    "def hist_gradient_boosting_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "\n",
    "    def to_dense(X): # FeatureHasher gets sparse matrices by default\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False) #This avoids sparce matrices when encoding\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') \n",
    "    \n",
    "    model = HistGradientBoostingClassifier(\n",
    "        learning_rate=params['learning_rate'], \n",
    "        l2_regularization=params['l2_regularization'], \n",
    "        max_iter=params['max_iter'], \n",
    "        max_depth=params['max_depth'], \n",
    "        min_samples_leaf=params['min_samples_leaf'], \n",
    "        max_features=params['max_features'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([\n",
    "            ('to_dict', dict_tf), \n",
    "            ('hasher', hasher), \n",
    "            ('dense', FunctionTransformer(to_dense, validate=False))  \n",
    "        ]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67ac2aa8-800c-489d-8803-51a6c23d1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__learning_rate': loguniform(1e-3, 1e-1),\n",
    "    'model__l2_regularization' : loguniform(1e-4, 1e1),\n",
    "    'model__max_iter' : randint(100, 1000),\n",
    "    'model__max_leaf_nodes' : randint(10, 100),\n",
    "    'model__max_depth': [None] + list(range(3, 11)),\n",
    "    'model__min_samples_leaf': randint(1, 20),\n",
    "    'model__max_features' : uniform(0, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7af271f0-59b8-48ac-8731-5568db577271",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['HistGradientBoostingClassifier'] = {}\n",
    "models['HistGradientBoostingClassifier']['pipeline'] = hist_gradient_boosting_pipeline\n",
    "models['HistGradientBoostingClassifier']['hyperparams'] = hyperparams\n",
    "models['HistGradientBoostingClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eed326-73e7-4044-aaaf-f4b3ea586aea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11485d49-505e-45a5-8198-7a5d90c2cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.3,\n",
    "    'subsample': 1.0, \n",
    "    'colsample_bytree': 1.0, \n",
    "    'max_depth': 6, \n",
    "    'gamma': 0.0, \n",
    "    'reg_alpha': 0.0, \n",
    "    'reg_lambda': 1.0 \n",
    "}\n",
    "\n",
    "def xgb_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "\n",
    "    def to_dense(X): # FeatureHasher gets sparse matrices by default\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "        \n",
    "    hash_cols   = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols    = ['Embarked']\n",
    "    ohe_tf     = OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False) #This avoids sparce matrices when encoding\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict') \n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        learning_rate=params['learning_rate'], \n",
    "        subsample=params['subsample'], \n",
    "        colsample_bytree=params['colsample_bytree'], \n",
    "        max_depth=params['max_depth'], \n",
    "        gamma=params['gamma'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        random_state=107\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('hash',  Pipeline([\n",
    "            ('to_dict', dict_tf), \n",
    "            ('hasher', hasher), \n",
    "            ('dense', FunctionTransformer(to_dense, validate=False))  \n",
    "        ]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5abb72ec-d6f1-49c8-a460-5a8aace99ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'model__learning_rate': loguniform(1e-3, 0.3),\n",
    "    'model__n_estimators' : randint(100, 1000),\n",
    "    'model__max_depth': [None] + list(range(3, 10)),\n",
    "    'model__subsample' : uniform(0.5, 0.5),  # rango [0.5, 1.0]\n",
    "    'model__colsample_bytree' : uniform(0.5, 0.5),  # rango [0.5, 1.0]\n",
    "    'model__gamma' : loguniform(1e-8, 1.0),\n",
    "    'model__reg_alpha' : loguniform(1e-8, 10.0),\n",
    "    'model__reg_lambda' : loguniform(1e-8, 10.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f19e1e52-5da2-4872-9ffb-a7ebbb10b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['XGBClassifier'] = {}\n",
    "models['XGBClassifier']['pipeline'] = xgb_pipeline\n",
    "models['XGBClassifier']['hyperparams'] = hyperparams\n",
    "models['XGBClassifier']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b779d1b-846a-4775-a736-1409ae61fc50",
   "metadata": {},
   "source": [
    "## Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c7524e8-667e-4855-9b91-b490fafc4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Priors for Titanic\n",
    "#Empiric count\n",
    "n_total = len(y)\n",
    "n_survived = (y == 1).sum()\n",
    "n_died     = (y == 0).sum()\n",
    "#Smoothening parameters\n",
    "alpha = 1.0\n",
    "n_classes = 2\n",
    "#P(Clase) = (n_clase + α) / (n_total + α * n_clases), we avoid that any rare class falls to cero by adding 1. This is not the case, but it is a good practice\n",
    "priors_smoothed = [\n",
    "    (n_died     + alpha) / (n_total + alpha * n_classes),  #P(Y=0)\n",
    "    (n_survived + alpha) / (n_total + alpha * n_classes)   #P(Y=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcd043-f75f-4c72-bc1c-aaced22e1527",
   "metadata": {},
   "source": [
    "### GaussianNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d2085-2f91-4ea1-8705-a16e850f7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in param_search_space.keys():\n",
    "    if 'model__' in key:\n",
    "        print(key.replace('model__', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51f050ca-6ca2-4a7a-bc00-404c8ecaa02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "hyperparams = {\n",
    "    'n_features': 8,\n",
    "    'quantile_range': (25.0, 75.0),\n",
    "    'priors' : None,\n",
    "    'var_smoothing' : 1e-09\n",
    "}\n",
    "\n",
    "def gnb_classifier_pipeline(params):\n",
    "    def df_to_dicts(X):\n",
    "        return X[hash_cols].astype(str).to_dict(orient='records')\n",
    "\n",
    "    def to_dense(X): # FeatureHasher gets sparse matrices by default\n",
    "        return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "        \n",
    "    std_cols = ['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Family']\n",
    "    rob_cols = ['Fare']\n",
    "    hash_cols = ['Title', 'Deck', 'TicketPrefix']\n",
    "    cat_cols = ['Embarked']\n",
    "\n",
    "    ss_tf = StandardScaler()\n",
    "    rb_tf = RobustScaler(quantile_range=params['quantile_range'])\n",
    "    ohe_tf = OneHotEncoder(handle_unknown='ignore', drop='first')\n",
    "    dict_tf = FunctionTransformer(df_to_dicts, validate=False)\n",
    "    hasher = FeatureHasher(n_features=params['n_features'], input_type='dict')\n",
    "\n",
    "    model = GaussianNB(\n",
    "        priors=params['priors'],\n",
    "        var_smoothing=params['var_smoothing']\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('standard', ss_tf, std_cols),\n",
    "        ('robust', rb_tf, rob_cols),\n",
    "        ('hash',  Pipeline([\n",
    "            ('to_dict', dict_tf), \n",
    "            ('hasher', hasher), \n",
    "            ('dense', FunctionTransformer(to_dense, validate=False))  \n",
    "        ]), hash_cols),\n",
    "        ('ohe',    ohe_tf,    cat_cols),\n",
    "        ('num_passthrough', 'passthrough', make_column_selector(dtype_include=np.number)),\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    return Pipeline(steps=[('preproc', preprocessor), ('model', model)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aba0bb10-6314-44d6-a740-a1ab7d040eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search_space = {\n",
    "    'preproc__hash__hasher__n_features' : randint(4, 65),\n",
    "    'preproc__robust__quantile_range': [(1.0,99.0), (5.0,95.0), (10.0,90.0), (25.0,75.0)],\n",
    "    'model__priors': [None, priors_smoothed],\n",
    "    'model__var_smoothing': loguniform(1e-12, 1e-6)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "29b8e58b-58ea-4868-bdf0-a4ffa104a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['GaussianNB'] = {}\n",
    "models['GaussianNB']['pipeline'] = gnb_classifier_pipeline\n",
    "models['GaussianNB']['hyperparams'] = hyperparams\n",
    "models['GaussianNB']['param_search'] = param_search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d019e-8580-4f81-b416-2436a71129c4",
   "metadata": {},
   "source": [
    "### CategoricalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823062e-8523-409d-91d9-28f8ff7c596f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b36b0c-2b92-4a91-9ec7-777f007b9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperparameterSearch('GaussianNB', gnb_classifier_pipeline(hyperparams), param_search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a914c-cfc0-4b89-85a5-ded5814a7a2f",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10de2e-2ecc-421e-bd42-8633fd1f8bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0a71b2-6e5d-4d1d-98af-69188353fd97",
   "metadata": {},
   "source": [
    "# Hyperparameter search and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0899f9d9-ad51-4bd2-b9b0-adfd1cd9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperparameterSearch(name, pipeline, param_distributions, verbose=True):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=107)\n",
    "    \n",
    "    rand_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        error_score='raise',\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=100,               \n",
    "        scoring='accuracy',\n",
    "        cv=cv,\n",
    "        verbose=2,\n",
    "        random_state=107,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rand_search.fit(X, y)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Best hyperparameters for {name}:\\n{rand_search.best_params_}')\n",
    "        print(f'Best accuracy for {name}: {rand_search.best_score_:.3f}')\n",
    "    \n",
    "    return rand_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2991c18-edd8-42d9-b105-2db46ffa625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Felpipe\\Trabajo\\Ciencias de datos en general\\KaggleChallenges\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "+-----------------------------+---------------+----------+\n",
      "|            Model            | Training time | Accuracy |\n",
      "+-----------------------------+---------------+----------+\n",
      "|      LogisticRegression     |   6.9080 [s]  |  0.8373  |\n",
      "|       RidgeClassifier       |  10.3780 [s]  |  0.8361  |\n",
      "| PassiveAggressiveClassifier |   5.2185 [s]  |  0.8025  |\n",
      "|        SGDClassifier        |   5.9230 [s]  |  0.8227  |\n",
      "+-----------------------------+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = ['Model', 'Training time', 'Accuracy']\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    pipeline = models[model]['pipeline']\n",
    "    hyperparams = models[model]['hyperparams']\n",
    "    param_search_space = models[model]['param_search']\n",
    "    search_results = HyperparameterSearch(model, \n",
    "                                          pipeline(hyperparams), \n",
    "                                          param_search_space, \n",
    "                                          verbose=False)\n",
    "    stop = time.time()\n",
    "    best_model = search_results.best_estimator_\n",
    "    y_pred = best_model.predict(X)\n",
    "    table.add_row([model, f'{stop - start:.4f} [s]', f'{accuracy_score(y, y_pred):.4f}'])\n",
    "    models[model]['best_params'] = search_results.best_params_\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9ad79-1420-416f-af63-4986424b57a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376505eb-1520-4b5e-86a0-bbbd9c54b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'LogisticRegression'\n",
    "pipeline = models[model]['pipeline']\n",
    "hyperparams = models[model]['hyperparams']\n",
    "\n",
    "best_hyperparams = hyperparams.copy()\n",
    "for hyperparam, value in models[model]['best_params'].items():\n",
    "    if 'model__' in hyperparam:\n",
    "        best_hyperparams[hyperparam.replace('model__', '')] = value.item() if type(value) == np.float64 else value\n",
    "    elif 'preproc__' in hyperparam:\n",
    "        if 'hash' in hyperparam:\n",
    "            best_hyperparams['n_features'] = value\n",
    "        elif 'robust' in hyperparam:\n",
    "            best_hyperparams['quantile_range'] = value\n",
    "print(hyperparams)\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cec5bef-e24f-4cc7-8343-e785620d8f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0:00:00\n",
      "Accuracy CV by fold: [0.82122905 0.83146067 0.79213483 0.80898876 0.87078652]\n",
      "Accuracy mean CV: 0.8249 ± 0.0264\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "scores = cross_val_score(\n",
    "    estimator=pipeline(best_hyperparams),\n",
    "    X=X, \n",
    "    y=y,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "stop = time.time()\n",
    "seconds_elapsed = stop - start\n",
    "time_format = str(dt.timedelta(seconds=int(seconds_elapsed)))\n",
    "    \n",
    "print(f'Training time: {time_format}')\n",
    "print(f'Accuracy CV by fold: {scores}')\n",
    "print(f'Accuracy mean CV: {scores.mean():.4f} ± {scores.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458751b3-e018-496b-874f-9117eae2fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=107,\n",
    "                                                    stratify=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
